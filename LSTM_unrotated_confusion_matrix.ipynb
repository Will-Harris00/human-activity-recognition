{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy import stats\n",
    "from scipy import integrate\n",
    "from IPython.display import HTML, display\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import t as the\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import tree\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 70)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define a list of file paths for the subject data files\n",
    "# list_of_files = ['PAMAP2_Dataset/Protocol/subject101.dat',\n",
    "#                  'PAMAP2_Dataset/Protocol/subject102.dat',\n",
    "#                  'PAMAP2_Dataset/Protocol/subject103.dat',\n",
    "#                  'PAMAP2_Dataset/Protocol/subject104.dat',\n",
    "#                  'PAMAP2_Dataset/Protocol/subject105.dat',\n",
    "#                  'PAMAP2_Dataset/Protocol/subject106.dat',\n",
    "#                  'PAMAP2_Dataset/Protocol/subject107.dat',\n",
    "#                  'PAMAP2_Dataset/Protocol/subject108.dat',\n",
    "#                  'PAMAP2_Dataset/Protocol/subject109.dat' ]\n",
    "\n",
    "subjectID = [1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "activityID = {0: 'transient',\n",
    "              1: 'lying',\n",
    "              2: 'sitting',\n",
    "              3: 'standing',\n",
    "              4: 'walking',\n",
    "              5: 'running',\n",
    "              6: 'cycling',\n",
    "              7: 'Nordic_walking',\n",
    "              9: 'watching_TV',\n",
    "              10: 'computer_work',\n",
    "              11: 'car driving',\n",
    "              12: 'ascending_stairs',\n",
    "              13: 'descending_stairs',\n",
    "              16: 'vacuum_cleaning',\n",
    "              17: 'ironing',\n",
    "              18: 'folding_laundry',\n",
    "              19: 'house_cleaning',\n",
    "              20: 'playing_soccer',\n",
    "              24: 'rope_jumping' }\n",
    "\n",
    "# Protocol Activities: lie, sit, stand, walk, run, cycle, Nordic walk, ascend and descend stairs, vacuum cleaning, ironing, and rope jump\n",
    "protocol_acts = [1,2,3,4,5,6,7,12,13,16,17,24]\n",
    "\n",
    "# Optional Activities: watching TV, computer work, car driving, folding laundry, house cleaning, playing soccer\n",
    "optional_acts = [9,10,11,18,19,20]\n",
    "\n",
    "# MET Classification of activities\n",
    "# lying, sitting, standing and ironing\n",
    "light_acts = [1,2,3,17]\n",
    "# vacuum cleaning, descending stairs, walking, Nordic walking and cycling\n",
    "mod_acts = [16,13,4,7,6]\n",
    "# ascending stairs, running and rope jumping\n",
    "vig_acts = [12,5,24]\n",
    "\n",
    "colNames = [\"timestamp\", \"activityID\",\"heartrate\"]\n",
    "\n",
    "IMUhand = ['handTemperature',\n",
    "           'handAcc16_1', 'handAcc16_2', 'handAcc16_3',\n",
    "           'handAcc6_1', 'handAcc6_2', 'handAcc6_3',\n",
    "           'handGyro1', 'handGyro2', 'handGyro3',\n",
    "           'handMagne1', 'handMagne2', 'handMagne3',\n",
    "           'handOrientation1', 'handOrientation2',\n",
    "           'handOrientation3', 'handOrientation4']\n",
    "\n",
    "IMUchest = ['chestTemperature',\n",
    "            'chestAcc16_1', 'chestAcc16_2', 'chestAcc16_3',\n",
    "            'chestAcc6_1', 'chestAcc6_2', 'chestAcc6_3',\n",
    "            'chestGyro1', 'chestGyro2', 'chestGyro3',\n",
    "            'chestMagne1', 'chestMagne2', 'chestMagne3',\n",
    "            'chestOrientation1', 'chestOrientation2',\n",
    "            'chestOrientation3', 'chestOrientation4']\n",
    "\n",
    "IMUankle = ['ankleTemperature',\n",
    "            'ankleAcc16_1', 'ankleAcc16_2', 'ankleAcc16_3',\n",
    "            'ankleAcc6_1', 'ankleAcc6_2', 'ankleAcc6_3',\n",
    "            'ankleGyro1', 'ankleGyro2', 'ankleGyro3',\n",
    "            'ankleMagne1', 'ankleMagne2', 'ankleMagne3',\n",
    "            'ankleOrientation1', 'ankleOrientation2',\n",
    "            'ankleOrientation3', 'ankleOrientation4']\n",
    "\n",
    "columns = colNames + IMUhand + IMUchest + IMUankle  # all columns in one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fields = [\n",
    "    \"timestamp\",\n",
    "    \"activityID\",\n",
    "    \"handAcc16_1\",\n",
    "    \"handAcc16_2\",\n",
    "    \"handAcc16_3\",\n",
    "    \"handGyro1\",\n",
    "    \"handGyro2\",\n",
    "    \"handGyro3\",\n",
    "    \"chestAcc16_1\",\n",
    "    \"chestAcc16_2\",\n",
    "    \"chestAcc16_3\",\n",
    "    \"chestGyro1\",\n",
    "    \"chestGyro2\",\n",
    "    \"chestGyro3\",\n",
    "    \"ankleAcc16_1\",\n",
    "    \"ankleAcc16_2\",\n",
    "    \"ankleAcc16_3\",\n",
    "    \"ankleGyro1\",\n",
    "    \"ankleGyro2\",\n",
    "    \"ankleGyro3\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to extract the features and labels from the subject data file\n",
    "def load_data(file_path):\n",
    "    frames = []\n",
    "    # Read the subject data file into a pandas DataFrame\n",
    "    for file in tqdm(os.listdir(file_path)):\n",
    "        if file.endswith(\".dat\"):\n",
    "            # Feed all column names to dataframe reader so that we can select only activityID and the accelerometer and gyroscope data\n",
    "            input_data = pd.read_table(\n",
    "                os.path.join(file_path, file), names=columns, usecols=fields, header=None, sep=\"\\s+\"\n",
    "            )\n",
    "            subject_id = int(file[-5])\n",
    "            input_data.insert(0, 'subjectID', subject_id) # prepend subject_id column from filename\n",
    "\n",
    "            # Keep only protocol activities\n",
    "            input_data = input_data[input_data['activityID'].isin(protocol_acts)]\n",
    "            input_data['activityID'] = input_data['activityID'].astype(int)\n",
    "\n",
    "            frames.append(input_data)\n",
    "\n",
    "    sensor_data = pd.concat(frames, ignore_index=True)\n",
    "    sensor_data.reset_index(drop=True, inplace=True)\n",
    "    print(sensor_data.shape)\n",
    "\n",
    "    return sensor_data\n",
    "\n",
    "sensor_data = load_data(\"PAMAP2_Dataset/Protocol/\")\n",
    "sensor_data.head(100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As a fist cleanup step, the activity ID 0, which corresponds to a transient period between activities is removed from the dataframe.\n",
    "\n",
    "The next step is keeping only the activities that correspond to each subject as documented. This is to avoid any unexpected or invalid activity data from affecting results.\n",
    "\n",
    "The data is then linearly interpolated to account for missing data in some rows, such as the HR column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "drop_index = []\n",
    "\n",
    "# Keep only activities as documented on file \"PerformedActivitiesSummary.pdf\"\n",
    "drop_index += list(\n",
    "    sensor_data.index[\n",
    "        (sensor_data[\"subjectID\"] == 1)\n",
    "        & (sensor_data[\"activityID\"].isin([10, 20]))\n",
    "    ]\n",
    ")\n",
    "drop_index += list(\n",
    "    sensor_data.index[\n",
    "        (sensor_data[\"subjectID\"] == 2)\n",
    "        & (sensor_data[\"activityID\"].isin([9, 10, 11, 18, 19, 20]))\n",
    "    ]\n",
    ")\n",
    "drop_index += list(\n",
    "    sensor_data.index[\n",
    "        (sensor_data[\"subjectID\"] == 3)\n",
    "        & (sensor_data[\"activityID\"].isin([5, 6, 7, 9, 10, 11, 18, 19, 20, 24]))\n",
    "    ]\n",
    ")\n",
    "drop_index += list(\n",
    "    sensor_data.index[\n",
    "        (sensor_data[\"subjectID\"] == 4)\n",
    "        & (sensor_data[\"activityID\"].isin([5, 9, 10, 11, 18, 19, 20, 24]))\n",
    "    ]\n",
    ")\n",
    "drop_index += list(\n",
    "    sensor_data.index[\n",
    "        (sensor_data[\"subjectID\"] == 5)\n",
    "        & (sensor_data[\"activityID\"].isin([9, 11, 18, 20]))\n",
    "    ]\n",
    ")\n",
    "drop_index += list(\n",
    "    sensor_data.index[\n",
    "        (sensor_data[\"subjectID\"] == 6)\n",
    "        & (sensor_data[\"activityID\"].isin([9, 11, 20]))\n",
    "    ]\n",
    ")\n",
    "drop_index += list(\n",
    "    sensor_data.index[\n",
    "        (sensor_data[\"subjectID\"] == 7)\n",
    "        & (sensor_data[\"activityID\"].isin([9, 10, 11, 18, 19, 20, 24]))\n",
    "    ]\n",
    ")\n",
    "drop_index += list(\n",
    "    sensor_data.index[\n",
    "        (sensor_data[\"subjectID\"] == 8)\n",
    "        & (sensor_data[\"activityID\"].isin([9, 11]))\n",
    "    ]\n",
    ")\n",
    "drop_index += list(\n",
    "    sensor_data.index[\n",
    "        (sensor_data[\"subjectID\"] == 9)\n",
    "        & (\n",
    "            sensor_data[\"activityID\"].isin(\n",
    "                [1, 2, 3, 4, 5, 6, 7, 9, 11, 12, 13, 16, 17]\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(len(drop_index))\n",
    "\n",
    "# Getting indexes of activity 0\n",
    "drop_index += list(sensor_data.index[sensor_data[\"activityID\"] == 0])\n",
    "\n",
    "sensor_data = sensor_data.drop(drop_index)\n",
    "# Interpolate data\n",
    "sensor_data = sensor_data.interpolate()\n",
    "print('Observe the removal of miss-classified activities and transient activities with ID zero: %i items' % (len(drop_index)))\n",
    "print(sensor_data.shape)\n",
    "sensor_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Customize the x-axis labels\n",
    "subject_labels = [\n",
    "    '101 (27M, 182cm, 83kg)',\n",
    "    '102 (25F, 169cm, 78kg)',\n",
    "    '103 (31M, 187cm, 92kg)',\n",
    "    '104 (24M, 194cm, 95kg)',\n",
    "    '105 (26M, 180cm, 73kg)',\n",
    "    '106 (26M, 183cm, 69kg)',\n",
    "    '107 (23M, 173cm, 86kg)',\n",
    "    '108 (32M, 179cm, 87kg)',\n",
    "    '109 (31M, 168cm, 65kg)'\n",
    "]\n",
    "\n",
    "# Compute the cross-tabulation of 'subjectID' and 'activity' without modifying sensor_data\n",
    "subject_counts = sensor_data.groupby(['subjectID', sensor_data['activityID'].replace(activityID)]).size().unstack(fill_value=0)\n",
    "\n",
    "# Rename the index and columns for better readability, adding '10' to the subjectIDs using a lambda function\n",
    "subject_counts = subject_counts.rename(index=lambda x: '10' + str(x))\n",
    "\n",
    "# Rename the index and columns for better readability\n",
    "subject_counts.index.name = 'subject'\n",
    "subject_counts.columns.name = 'activity'\n",
    "\n",
    "# print(subject_counts.head(10))\n",
    "\n",
    "# Define a custom color palette using hexadecimal color codes\n",
    "custom_palette = ['#a6cee3', '#1f78b4', '#b2df8a', '#33a02c', '#fb9a99', '#e31a1c', '#fdbf6f', '#ff7f00',\n",
    "                  '#cab2d6', '#6a3d9a', '#ffff99', '#b15928']\n",
    "\n",
    "# Create the stacked bar plot using Seaborn\n",
    "sns.set(style='whitegrid', palette=custom_palette)\n",
    "plt.figure(figsize=(10, 6))\n",
    "subject_counts.plot(kind='bar', stacked=True)\n",
    "plt.xlabel('Subject ID (Description: Age, Sex, Height, Weight)')\n",
    "plt.ylabel('Number of Instances')\n",
    "# plt.title('Activity Distribution by Subject')\n",
    "plt.legend(title='Activity', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=45, ha='right')  # Keep 45-degree rotation and horizontal alignment\n",
    "plt.xticks(range(len(subject_labels)), subject_labels)  # Use subject_labels as x-axis labels\n",
    "# Set the y-axis limits to include 30,000\n",
    "plt.ylim(0, 300000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Compute the cross-tabulation of 'activity' and 'subjectID' without modifying sensor_data\n",
    "activity_counts = sensor_data.groupby([sensor_data['activityID'].replace(activityID), 'subjectID']).size().unstack(fill_value=0).rename(columns=lambda x: '10' + str(x))\n",
    "\n",
    "# # Rename the index and columns for better readability\n",
    "activity_counts.index.name = 'activity'\n",
    "activity_counts.columns.name = 'subject'\n",
    "\n",
    "# print(activity_counts.head(10))\n",
    "\n",
    "# Define a custom color palette using hexadecimal color codes\n",
    "custom_palette = ['#973547', '#a6cee3', '#e6798a', '#ca8f45', '#6aaf4f', '#5ba9b3', '#5975e6', '#b385ed', '#e46dcf']\n",
    "\n",
    "# Create the stacked bar plot using Seaborn\n",
    "sns.set(style='whitegrid', palette=custom_palette)\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = activity_counts.plot(kind='bar', stacked=True)\n",
    "plt.xlabel('Activity')\n",
    "plt.ylabel('Number of Instances')\n",
    "# plt.title('Subject Distribution by Activity')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels\n",
    "plt.legend(title='Subject ID', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.subplots_adjust(right=0.8)  # Adjust the figure size to accommodate the legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Latest Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sensor_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sensor_data.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "Number of samples in windowed dataset: 3885300\n",
      "Number of discarded window samples: 300\n",
      "(38853, 100, 18)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def prepare_data(sensor_data, window_size, stride):\n",
    "    features = sensor_data[['handAcc16_1', 'handAcc16_2', 'handAcc16_3',\n",
    "                            'handGyro1', 'handGyro2', 'handGyro3',\n",
    "                            'chestAcc16_1', 'chestAcc16_2', 'chestAcc16_3',\n",
    "                            'chestGyro1', 'chestGyro2', 'chestGyro3',\n",
    "                            'ankleAcc16_1', 'ankleAcc16_2', 'ankleAcc16_3',\n",
    "                            'ankleGyro1', 'ankleGyro2', 'ankleGyro3']].values\n",
    "\n",
    "    windows = []\n",
    "    subjects = []\n",
    "    activities = []\n",
    "    windowed_samples = 0\n",
    "    discarded_samples = 0\n",
    "\n",
    "    majority_threshold = 0.50  # Set the majority threshold\n",
    "\n",
    "    num_samples = len(sensor_data)\n",
    "\n",
    "    for i in range(0, num_samples - window_size + 1, stride):\n",
    "        window = features[i:i + window_size]\n",
    "\n",
    "        if len(window) == window_size:\n",
    "            activity_counts = sensor_data.iloc[i:i + window_size]['activityID'].value_counts()\n",
    "            subject_counts = sensor_data.iloc[i:i + window_size]['subjectID'].value_counts()\n",
    "            majority_activity = activity_counts.idxmax()\n",
    "            majority_subject = subject_counts.idxmax()\n",
    "            majority_activity_percentage = activity_counts.max() / window_size\n",
    "\n",
    "            if majority_activity_percentage > majority_threshold:\n",
    "                windows.append(window)\n",
    "                subjects.append(majority_subject)\n",
    "                activities.append(majority_activity)\n",
    "                windowed_samples += len(window)\n",
    "            else:\n",
    "                discarded_samples += len(window)\n",
    "                print(majority_activity_percentage)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    windows = np.array(windows)\n",
    "    subjects = np.array(subjects)\n",
    "    activities = np.array(activities)\n",
    "\n",
    "    # # Encode activity and subject labels\n",
    "    # label_encoder = LabelEncoder()\n",
    "    # activities = label_encoder.fit_transform(activities)\n",
    "    # subjects = label_encoder.fit_transform(subjects)\n",
    "\n",
    "    return windows, subjects, activities, windowed_samples, discarded_samples\n",
    "\n",
    "# Step 1: Read the data and windowing\n",
    "window_size = 100\n",
    "stride = 50\n",
    "\n",
    "windows, subjects, activities, windowed_samples, discarded_samples = prepare_data(sensor_data, window_size, stride)\n",
    "\n",
    "print(\"Number of samples in windowed dataset:\", windowed_samples)\n",
    "print(\"Number of discarded window samples:\", discarded_samples)\n",
    "print(windows.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique sensor_data entries that are used in windows without being discarded: 1942850\n",
      "Number of original raw samples from sensor_data: 1942871\n",
      "True number of discarded samples: 21\n",
      "Number of windows: 38853\n",
      "(38853, 100, 18)\n"
     ]
    }
   ],
   "source": [
    "# Flatten the windows array\n",
    "flattened_windows = windows.reshape(-1, windows.shape[-1])\n",
    "\n",
    "# Use np.unique to get the unique rows and their counts\n",
    "unique_rows, counts = np.unique(flattened_windows, axis=0, return_counts=True)\n",
    "\n",
    "# Calculate the number of unique instances in sensor_data that occurs in all the windows\n",
    "num_unique_instances = len(unique_rows)\n",
    "\n",
    "# # Flatten the windows array and reshape it to have 21 columns\n",
    "# flattened_windows = windows.reshape(-1, windows.shape[-1])\n",
    "#\n",
    "# # Get the indices of the unique rows in flattened_windows\n",
    "# unique_indices = np.unique(flattened_windows, axis=0, return_index=True)[1]\n",
    "#\n",
    "# # Extract the unique instances from sensor_data based on the unique indices\n",
    "# unique_instances = sensor_data.iloc[unique_indices]\n",
    "#\n",
    "# # Calculate the number of unique instances from sensor_data that are used in the windows\n",
    "# num_unique_instances = len(unique_instances)\n",
    "#\n",
    "# print(\"Number of unique instances in sensor_data that are used in the windows:\", num_unique_instances)\n",
    "\n",
    "\n",
    "# Because of the stride this can be different from the number of discarded samples.\n",
    "print(\"Number of unique sensor_data entries that are used in windows without being discarded:\", num_unique_instances)\n",
    "print(\"Number of original raw samples from sensor_data:\", len(sensor_data))\n",
    "\n",
    "# With only 21 unique samples not represented in the windowed data we have a good coverage of the original data.\n",
    "print(\"True number of discarded samples:\", len(sensor_data) - num_unique_instances)\n",
    "print(\"Number of windows:\", windows.shape[0])\n",
    "print(windows.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Window:\n",
      "handAcc16_1: [-9.64938 -9.80348 -9.65232 -9.65045 -9.54564 -9.50544 -9.53762 -9.73084\n",
      " -9.72923 -9.5738  -9.53868 -9.42371 -9.69091 -9.64777 -9.69412 -9.38779\n",
      " -9.46658 -9.65392 -9.65419 -9.66462 -9.4623  -9.66087 -9.53788 -9.69037\n",
      " -9.69492 -9.53467 -9.61185 -9.61052 -9.68289 -9.61827 -9.57567 -9.57032\n",
      " -9.57487 -9.53494 -9.6172  -9.60891 -9.65258 -9.57647 -9.65232 -9.68369\n",
      " -9.65178 -9.54002 -9.46043 -9.65553 -9.65366 -9.54002 -9.72977 -9.69171\n",
      " -9.57674 -9.65232 -9.46819 -9.61025 -9.6156  -9.53842 -9.5336  -9.57781\n",
      " -9.65205 -9.53895 -9.57647 -9.65686 -9.61506 -9.53467 -9.65446 -9.5328\n",
      " -9.57621 -9.57861 -9.51159 -9.61399 -9.6164  -9.50384 -9.65499 -9.53414\n",
      " -9.5754  -9.50544 -9.50143 -9.61266 -9.61079 -9.61586 -9.61159 -9.42612\n",
      " -9.58102 -9.46097 -9.61587 -9.69359 -9.57541 -9.57139 -9.57407 -9.45883\n",
      " -9.64858 -9.56952 -9.57808 -9.57674 -9.54644 -9.64644 -9.64965 -9.57541\n",
      " -9.61266 -9.57808 -9.65419 -9.57594]\n",
      "handAcc16_2: [-1.68584 -1.72328 -1.53509 -1.64823 -1.53764 -1.5752  -1.53563 -1.76247\n",
      " -1.7242  -1.68642 -1.68738 -1.64998 -1.83797 -1.64756 -1.72516 -1.53713\n",
      " -1.61309 -1.76272 -1.80066 -1.57604 -1.57415 -1.80234 -1.57357 -1.76209\n",
      " -1.64962 -1.68638 -1.72407 -1.53437 -1.6466  -1.68781 -1.76263 -1.76129\n",
      " -1.64882 -1.72432 -1.72541 -1.68546 -1.76239 -1.68709 -1.72445 -1.76042\n",
      " -1.64857 -1.68772 -1.6873  -1.61164 -1.72478 -1.68772 -1.80008 -1.76243\n",
      " -1.72503 -1.72445 -1.65137 -1.68579 -1.68713 -1.64944 -1.72398 -1.68743\n",
      " -1.68651 -1.72532 -1.68709 -1.80133 -1.61126 -1.68638 -1.64924 -1.61016\n",
      " -1.64915 -1.80124 -1.69036 -1.64886 -1.61159 -1.53692 -1.72512 -1.6105\n",
      " -1.7247  -1.5752  -1.57419 -1.64852 -1.57231 -1.72507 -1.68613 -1.61272\n",
      " -1.57462 -1.57381 -1.53571 -1.64928 -1.53533 -1.53433 -1.535   -1.64902\n",
      " -1.57202 -1.64747 -1.53601 -1.53567 -1.65146 -1.64723 -1.53442 -1.53533\n",
      " -1.64852 -1.53601 -1.6113  -1.61121]\n",
      "handAcc16_3: [0.978888 0.899731 0.862873 0.940257 0.672043 0.749746 0.90308  0.823976\n",
      " 0.862359 0.980022 0.903576 0.94366  0.901804 1.01727  0.785913 0.866842\n",
      " 0.788943 0.863617 0.863741 0.51644  0.904338 0.671209 0.903204 0.901556\n",
      " 0.747159 1.0191   0.979579 0.978959 1.09372  0.786923 0.941763 1.09579\n",
      " 0.941391 1.01922  0.825554 1.05647  0.902123 0.903009 0.901999 1.09409\n",
      " 0.901751 0.86507  0.981723 0.786108 0.863493 0.86507  0.862607 0.86305\n",
      " 0.903133 0.901999 0.750561 1.01796  0.863936 0.903452 1.05773  0.864503\n",
      " 0.901875 0.9037   0.903009 0.786728 0.863688 1.0191   0.824739 1.05735\n",
      " 0.902885 0.864875 0.596093 0.902318 0.825182 0.788128 0.824987 1.01885\n",
      " 0.941639 0.749746 0.865265 0.940824 0.979083 0.86406  0.979455 0.866523\n",
      " 0.748612 0.942845 0.824934 0.785665 0.902513 1.01803  0.941019 1.02011\n",
      " 0.978516 1.09542  0.825501 0.864007 0.672415 1.05578  0.939885 0.902513\n",
      " 0.940824 0.825501 0.824615 0.902761]\n",
      "handGyro1: [-0.00824009  0.0542927   0.00032358  0.00970056 -0.0269928  -0.0108631\n",
      " -0.0478867  -0.0320727   0.00053286  0.037642    0.0108471   0.022249\n",
      "  0.0120581   0.0262456   0.00908412 -0.0387346  -0.0208589  -0.0152008\n",
      "  0.00162211  0.0226717   0.00696036  0.0114591  -0.00349657  0.00801404\n",
      "  0.0277579   0.0230757   0.00039872 -0.00834351  0.0358333   0.0569555\n",
      "  0.0613327   0.0373383   0.00371865  0.0288686   0.0216805  -0.0189935\n",
      " -0.032841   -0.00101884 -0.011679    0.0290578   0.00793399  0.0079811\n",
      "  0.0170507   0.0483116   0.0190524   0.012681    0.00429473  0.0340448\n",
      "  0.041612    0.0571633   0.0574751   0.054761    0.0361546   0.053082\n",
      "  0.0191873   0.019874    0.052205    0.0244702   0.0253862   0.0443593\n",
      "  0.0162028   0.0131072   0.0236319   0.0262072   0.032384    0.0186751\n",
      " -0.0456719   0.00973954  0.00864426 -0.0218727   0.0455993   0.0358994\n",
      " -0.00282152 -0.0164615   0.00556789 -0.0138026   0.0279521  -0.00548838\n",
      "  0.0330774   0.011842    0.00815645 -0.0110531  -0.020883    0.0150366\n",
      " -0.0181007   0.0429025   0.0187505   0.017404   -0.0287809  -0.0399042\n",
      " -0.0273805   0.00899534 -0.0334868  -0.0405569   0.00823955 -0.0482675\n",
      " -0.0185566  -0.040589   -0.103363   -0.0600638 ]\n",
      "handGyro2: [-2.90035e-02 -1.86092e-03 -4.25006e-02 -1.26599e-02 -3.30023e-03\n",
      " -8.35816e-03  5.50720e-03 -5.73965e-03 -3.84491e-02 -2.92234e-02\n",
      " -6.45603e-03 -2.89645e-02 -4.22167e-02  3.33200e-03 -7.18492e-03\n",
      " -3.90894e-02 -4.46448e-02 -5.54968e-03 -2.54076e-02 -1.87984e-02\n",
      " -9.94171e-03 -2.26026e-02  1.24439e-02 -1.59452e-02 -1.12409e-02\n",
      " -8.40926e-07  8.72535e-03 -1.73802e-02  1.87579e-02 -3.24100e-02\n",
      " -3.83535e-02  5.03848e-02 -6.81525e-03 -1.73133e-02 -2.81691e-03\n",
      " -8.52556e-03 -4.21656e-02 -5.17530e-02 -2.15322e-02 -4.79303e-02\n",
      " -2.89893e-02 -4.97768e-02 -6.62159e-03 -3.62612e-02 -2.99991e-02\n",
      " -7.25755e-03 -4.22627e-03 -2.23821e-02 -2.56405e-02 -2.52694e-02\n",
      " -2.79123e-02 -2.93321e-02 -7.09097e-03 -3.91969e-02 -4.70161e-03\n",
      " -1.35468e-02  1.19628e-02  9.75461e-04 -1.98430e-02 -3.24738e-02\n",
      " -2.32400e-02 -1.68070e-02  7.54000e-03  5.84240e-03  7.97512e-03\n",
      " -1.57788e-02 -4.98695e-03  1.76813e-02  8.13022e-03 -1.32290e-02\n",
      " -1.22228e-02 -2.05253e-03 -7.26206e-03  1.93961e-02  7.77162e-03\n",
      "  1.10271e-02  3.69815e-02 -2.51651e-02 -2.52068e-03 -2.53278e-03\n",
      "  2.77277e-03  2.71877e-02  2.02676e-02  1.51282e-02  3.49437e-02\n",
      " -3.80778e-03  2.86785e-02  1.98582e-03  3.04961e-02  2.32987e-02\n",
      "  3.73882e-03  2.90599e-02 -8.53274e-03  1.89619e-02 -1.43421e-03\n",
      "  4.12697e-03  2.87265e-02 -1.97987e-02 -4.76685e-02  9.02567e-06]\n",
      "handGyro3: [ 2.53635e-03 -2.33454e-02 -3.70606e-02  7.47625e-03  6.46357e-03\n",
      " -5.18741e-03  5.63941e-03  1.21677e-02 -1.18592e-02 -4.39188e-03\n",
      " -1.06916e-02  1.46342e-02 -4.30987e-03  6.88210e-04 -5.72462e-03\n",
      " -3.99674e-02  1.94142e-02  3.82977e-02  5.28803e-03 -1.09356e-02\n",
      "  1.67198e-02  9.68505e-03 -1.36112e-02  1.06906e-02  1.69495e-02\n",
      "  3.31275e-02  5.24746e-04 -4.32167e-04  7.99619e-03 -1.09273e-02\n",
      " -1.69112e-02  3.15980e-02  1.09292e-02  8.41197e-03  2.29912e-02\n",
      "  2.39469e-02 -4.05903e-03 -8.13090e-03  2.73263e-03  1.99600e-02\n",
      "  2.28024e-02 -2.93598e-03  9.43837e-03  6.38304e-03  9.54545e-03\n",
      "  1.93879e-02 -1.65544e-02  3.30236e-03  1.62481e-03 -2.48992e-02\n",
      "  8.74573e-05  1.08689e-02  1.05083e-02  1.80489e-02 -1.30329e-02\n",
      " -7.75482e-04  2.38670e-02  7.27373e-03 -2.01255e-02 -5.24468e-03\n",
      " -1.61771e-02  6.55127e-03  3.29671e-03  7.68748e-04 -1.66293e-02\n",
      " -2.00819e-02 -1.22180e-02 -1.41049e-02  3.95561e-03  7.04552e-03\n",
      "  4.38841e-04 -1.85783e-02 -3.09505e-02  3.99141e-03 -6.95891e-03\n",
      "  2.08477e-03 -2.32712e-02 -1.15090e-02 -3.58949e-03 -2.05792e-02\n",
      " -7.92207e-03 -1.99188e-02 -2.19417e-02  1.09498e-02 -7.22646e-03\n",
      "  4.01766e-03 -1.53034e-02 -1.20047e-02 -1.00163e-02  8.82480e-03\n",
      "  1.92197e-02 -2.54457e-02  2.86745e-03  3.66269e-03 -8.89239e-03\n",
      " -1.36289e-02 -1.99671e-02 -6.78056e-03 -6.82892e-03 -3.88869e-03]\n",
      "chestAcc16_1: [-0.127523   0.103751  -0.016212  -0.0533821 -0.0432874 -0.0995031\n",
      " -0.0543267 -0.0188981 -0.0184996 -0.0120354 -0.08901   -0.16877\n",
      " -0.20967   -0.133043  -0.014323  -0.0960718 -0.166036  -0.162306\n",
      "  0.0298093 -0.12608   -0.0950276 -0.0929393 -0.092242  -0.168969\n",
      " -0.0581564 -0.167626  -0.0893088 -0.016212   0.0181724 -0.0929393\n",
      " -0.0573115  0.0966892 -0.0841876 -0.0897073 -0.128667  -0.0906518\n",
      " -0.0490063 -0.0579572 -0.0559685 -0.0893088 -0.0546255 -0.0547251\n",
      " -0.13001   -0.131154  -0.0902533 -0.0130796 -0.209272   0.0274222\n",
      " -0.165339  -0.16708   -0.0911979 -0.0105928 -0.0487074 -0.12618\n",
      " -0.0955257 -0.170213  -0.0568134 -0.13001   -0.124291  -0.0917956\n",
      " -0.166781  -0.0884639 -0.0959722 -0.164693  -0.126479  -0.0905522\n",
      " -0.134286  -0.056913  -0.0534817 -0.0179535 -0.0223294 -0.0489067\n",
      " -0.135529   0.0205596 -0.136773   0.0663817 -0.0847337 -0.0951273\n",
      " -0.245597  -0.0525372  0.0229467 -0.0248161 -0.0512938 -0.0512938\n",
      " -0.0522383 -0.090353  -0.010792  -0.133043  -0.0961714  0.0982314\n",
      " -0.0511942  0.134804   0.0259796  0.0565859 -0.131154   0.0908707\n",
      " -0.05015   -0.0531829 -0.0164113 -0.0211856]\n",
      "chestAcc16_2: [ 9.77587  9.88863  9.92535  9.85042  9.77744  9.84872  9.92546  9.77421\n",
      "  9.92483  9.77577  9.92636  9.88817  9.6995   9.84988  9.77525  9.8495\n",
      "  9.62537  9.73911  9.88939  9.88909  9.8121   9.73732 10.0009   9.81286\n",
      "  9.77407  9.88843  9.8134   9.92535  9.81149  9.73732  9.66137  9.81177\n",
      "  9.58878  9.66279  9.77561  9.73783  9.77615  9.84938  9.73694  9.8134\n",
      "  9.8125   9.77485  9.70004  9.69978  9.88845  9.81316  9.85011  9.85121\n",
      "  9.88894  9.66277  9.9635   9.88899  9.88911  9.85143  9.62384  9.77495\n",
      "  9.84964  9.70004  9.70134  9.73758  9.77573  9.7007   9.88715  9.70094\n",
      "  9.73847  9.77549  9.81196  9.81199  9.81276  9.69917  9.77344  9.8138\n",
      "  9.77405  9.84966  9.73614  9.73839  9.81444  9.77445  9.66249  9.73771\n",
      "  9.88783  9.69761  9.77563  9.77563  9.85068  9.85079  9.81368  9.84988\n",
      "  9.81185  9.96264  9.81328  9.81165  9.738    9.92433  9.69978  9.77282\n",
      "  9.77589  9.92572  9.85004  9.77369]\n",
      "chestAcc16_3: [ 0.124375    0.20564     0.00974246  0.0480336   0.396942   -0.223843\n",
      "  0.00905157 -0.0672568  -0.0677392   0.165188    0.124583    0.00709946\n",
      " -0.0704702  -0.0695707   0.0877064  -0.107621    0.124166    0.240027\n",
      "  0.28174     0.162754   -0.0687592   0.0089636   0.0081195   0.00734063\n",
      " -0.106689    0.0458403   0.124945    0.00974246 -0.105427    0.0089636\n",
      " -0.067586   -0.0265639   0.319373    0.125427    0.0856338   0.0864452\n",
      "  0.203238   -0.10693    -0.0290863   0.124945    0.00941332  0.00953391\n",
      "  0.0471341   0.00839328  0.0859629   0.126327   -0.0709525   0.204379\n",
      "  0.123322    0.0853046   0.0469809   0.203567    0.202876    0.162874\n",
      " -0.0681563  -0.0312796  -0.0681889   0.0471341   0.240838    0.0477044\n",
      "  0.0849429   0.164047   -0.107741    0.162666    0.163236    0.0863247\n",
      " -0.108191   -0.0680683   0.0481541  -0.0282749  -0.183479    0.203117\n",
      " -0.146811   -0.0280663  -0.185431    0.244172    0.279908   -0.0686386\n",
      "  0.00644118  0.0871361   0.0492948  -0.26072     0.125756    0.125756\n",
      "  0.0867744   0.0860835   0.203808   -0.0695707  -0.1075      0.0116946\n",
      "  0.125636   -0.025873    0.166      -0.105098    0.00839328 -0.220147\n",
      "  0.164497    0.0477924   0.00998363 -0.144738  ]\n",
      "chestGyro1: [-0.0254643  -0.00730361  0.0335239  -0.0305231  -0.0223739  -0.0206856\n",
      " -0.00871446 -0.0450581  -0.0452009   0.00229521 -0.00961019 -0.0307223\n",
      "  0.00136029  0.0385617  -0.0285948  -0.0373479  -0.0239441  -0.00605213\n",
      " -0.0325327  -0.0239943   0.00928295 -0.0176547  -0.0326206   0.00539248\n",
      " -0.0359043  -0.0412656  -0.0206352   0.03431     0.0516776  -0.0162886\n",
      " -0.0109925  -0.0403168  -0.0220372  -0.0301053   0.0325002   0.00948492\n",
      " -0.0144551   0.0325178   0.0213434  -0.0208258   0.00720642  0.0369822\n",
      " -0.00857266  0.0010357   0.0252488   0.0552221   0.00034332  0.0254608\n",
      "  0.0198735   0.0205207  -0.00412844  0.0577047   0.0484296  -0.00618404\n",
      "  0.0237422   0.0577936   0.0275311   0.0165776   0.0680023  -0.00375278\n",
      " -0.0154205   0.0134208   0.0197612  -0.0130497   0.00030303 -0.0203587\n",
      "  0.00863894  0.0137219  -0.00611758  0.0244813   0.0345106   0.0280035\n",
      " -0.00783422  0.0346715   0.00314262  0.0243186   0.0332548  -0.00657455\n",
      " -0.00908092  0.00623534  0.00674364 -0.0080672  -0.0155581   0.0081893\n",
      "  0.0290934   0.00067334  0.0485961   0.0785736   0.0306177  -0.0190118\n",
      "  0.00508997  0.0234758  -0.0352986  -0.00144015  0.00511536 -0.0221558\n",
      " -0.0129203   0.0300453   0.0127852  -0.0195068 ]\n",
      "chestGyro2: [ 0.021596   -0.0243699   0.00022097 -0.0345052  -0.0358403   0.00085243\n",
      " -0.027601   -0.0221005  -0.0394695   0.00244343  0.0275768   0.034535\n",
      "  0.034836    0.0268057  -0.0078032   0.0235619   0.0185885  -0.0254136\n",
      " -0.0127762   0.00169558 -0.0108537   0.0120961   0.0158434  -0.0237984\n",
      " -0.00226665  0.0274715   0.0291283  -0.00316982 -0.00148952 -0.013771\n",
      "  0.0074747  -0.017982   -0.0390252   0.00784707 -0.00281716 -0.0116127\n",
      "  0.0103561   0.0111021  -0.0245269  -0.0215532  -0.0612377  -0.0388915\n",
      " -0.0123475   0.00221074 -0.0225276  -0.0186531  -0.0238404  -0.0312943\n",
      " -0.00462643  0.0129769  -0.0278072  -0.0161081  -0.0124116  -0.0351599\n",
      "  0.0314858   0.0193226  -0.0213722  -0.0054447   0.00176847 -0.0195899\n",
      "  0.0119724   0.0211628  -0.0097376  -0.0361423  -0.0352261   0.0170581\n",
      "  0.0191736  -0.0212436   0.0141873  -0.00310217 -0.0039123   0.0121693\n",
      "  0.024267    0.00672544 -0.00346161 -0.0179707   0.0228352   0.0109847\n",
      " -0.0210435  -0.0339011   0.0182113  -0.00999768  0.00304848  0.026515\n",
      " -0.00876317 -0.0283184   0.0344407  -0.0172899   0.0130775  -0.0155762\n",
      "  0.0186245  -0.0263865  -0.00954736  0.0443752  -0.0209791   0.00936991\n",
      "  0.00353861  0.00054948 -0.0116003  -0.0280656 ]\n",
      "chestGyro3: [-2.86238e-02 -2.47935e-02 -2.09555e-02 -1.43126e-02 -2.64569e-02\n",
      " -1.77885e-02  7.65670e-03 -2.00951e-02 -4.83248e-02 -1.36998e-02\n",
      " -2.80931e-03  1.30882e-02 -3.14046e-02 -3.52321e-02  8.02306e-04\n",
      " -8.82730e-03 -5.35893e-03 -1.52311e-02 -3.18949e-02 -1.90632e-02\n",
      " -5.52304e-03 -7.61546e-03 -2.86870e-02 -1.31829e-02 -2.59060e-02\n",
      " -1.15982e-03 -5.90070e-05 -1.61876e-02 -1.63654e-02 -2.30710e-02\n",
      " -1.96028e-02  1.41352e-02  4.19794e-03 -3.70130e-02  3.37931e-03\n",
      " -4.26894e-02 -3.09833e-03  4.16976e-03 -1.56346e-02 -6.44402e-03\n",
      " -2.46344e-02  3.76419e-03  2.13534e-02 -3.86031e-02  4.14871e-03\n",
      " -1.83272e-02 -1.18547e-02 -4.10852e-02 -2.51952e-02 -4.14318e-02\n",
      " -1.78494e-02  1.29076e-02 -5.52063e-03  1.54793e-02 -2.64413e-02\n",
      " -2.91319e-02 -9.81592e-03  9.06184e-03 -1.40148e-02  1.60191e-02\n",
      " -2.19302e-03 -4.15161e-02 -1.71027e-02 -1.28826e-02  1.88481e-02\n",
      "  2.49591e-02  7.43093e-03 -1.64673e-02  8.95814e-03  1.47044e-02\n",
      " -1.94376e-02 -6.67128e-03 -1.37376e-03  1.60654e-02 -4.12368e-02\n",
      " -4.98648e-02 -4.11408e-02 -2.89451e-02  7.68730e-03 -1.95337e-03\n",
      "  1.32801e-02 -8.57994e-03 -6.20669e-03 -2.56395e-02  1.18124e-02\n",
      " -3.04570e-02 -4.51581e-02 -4.58621e-03 -1.01720e-02 -1.63280e-02\n",
      " -4.31543e-03 -6.16210e-03 -2.99981e-02 -1.90133e-02  6.22325e-03\n",
      " -3.13742e-02 -2.66675e-02 -3.51326e-02 -8.30597e-03 -4.29117e-02]\n",
      "ankleAcc16_1: [9.56066 9.55465 9.63151 9.55866 9.55846 9.67153 9.55606 9.52183 9.63391\n",
      " 9.59479 9.63351 9.55296 9.59129 9.63391 9.70526 9.67083 9.63591 9.59558\n",
      " 9.55536 9.55795 9.52214 9.55906 9.59518 9.63061 9.55866 9.59208 9.66473\n",
      " 9.62581 9.60108 9.70916 9.59078 9.55135 9.52163 9.59078 9.66913 9.59738\n",
      " 9.59168 9.55446 9.51633 9.4796  9.55956 9.56075 9.6343  9.59259 9.67173\n",
      " 9.74689 9.55446 9.54876 9.67122 9.59298 9.59829 9.59188 9.63131 9.55666\n",
      " 9.52143 9.51593 9.67243 9.70746 9.63312 9.59039 9.55906 9.62951 9.55646\n",
      " 9.59558 9.6251  9.44328 9.63391 9.67023 9.70695 9.56106 9.51903 9.44709\n",
      " 9.55756 9.59699 9.52253 9.66783 9.52123 9.63281 9.4842  9.58779 9.40456\n",
      " 9.51703 9.40306 9.59338 9.58948 9.58999 9.63371 9.51593 9.51573 9.77771\n",
      " 9.67003 9.44068 9.52423 9.74319 9.44918 9.62911 9.63202 9.55886 9.6286\n",
      " 9.51974]\n",
      "ankleAcc16_2: [-0.268664  -0.152831  -0.191916  -0.230053  -0.268022  -0.116859\n",
      " -0.305348  -0.11591   -0.15459   -0.267938  -0.230527  -0.266416\n",
      " -0.304943  -0.15459   -0.192069  -0.0406004 -0.193201  -0.116063\n",
      " -0.22909   -0.153795  -0.268106  -0.154116  -0.192001  -0.153627\n",
      " -0.230053  -0.153068  -0.1529    -0.228279  -0.117669  -0.0791273\n",
      " -0.190716  -0.151867  -0.153879  -0.190716  -0.154185  -0.192643\n",
      " -0.229005  -0.1908    -0.114304  -0.190326  -0.268343  -0.0405315\n",
      " -0.0786531 -0.267295  -0.0788902 -0.23156   -0.1908    -0.227162\n",
      "  0.0353368 -0.191358  -0.230933  -0.191037  -0.229885  -0.191442\n",
      " -0.191848  -0.190241  -0.155149  -0.192712  -0.306465  -0.266653\n",
      " -0.154116  -0.153305  -0.229411  -0.116063  -0.15202   -0.19041\n",
      " -0.15459   -0.154506  -0.0784849 -0.192727  -0.229174  -0.3056\n",
      " -0.229732  -0.26858   -0.192169  -0.191832  -0.229816  -0.154269\n",
      " -0.153642  -0.341948  -0.22782   -0.190563  -0.303436  -0.115421\n",
      " -0.228363  -0.34259   -0.192559  -0.190241  -0.22821   -0.22987\n",
      " -0.192475  -0.265704  -0.0785842 -0.306534  -0.116079  -0.229243\n",
      " -0.306143  -0.192085  -0.115016  -0.305432 ]\n",
      "ankleAcc16_3: [-2.50197 -2.73429 -2.65608 -2.57941 -2.57917 -2.57895 -2.65615 -2.58049\n",
      " -2.5791  -2.61739 -2.57864 -2.77219 -2.73297 -2.5791  -2.69391 -2.61802\n",
      " -2.50166 -2.61833 -2.69522 -2.61848 -2.54096 -2.57987 -2.61786 -2.69491\n",
      " -2.57941 -2.7339  -2.81034 -2.84886 -2.42531 -2.5788  -2.77227 -2.85009\n",
      " -2.58026 -2.77227 -2.65592 -2.54065 -2.73343 -2.73405 -2.77351 -2.73482\n",
      " -2.54057 -2.54197 -2.57957 -2.6946  -2.57919 -2.53888 -2.73405 -2.92683\n",
      " -2.61849 -2.69506 -2.50182 -2.73367 -2.65584 -2.65685 -2.58003 -2.77304\n",
      " -2.54012 -2.6167  -2.57817 -2.7718  -2.57987 -2.73352 -2.65661 -2.61833\n",
      " -2.88793 -2.69661 -2.5791  -2.61732 -2.65601 -2.50244 -2.657   -2.54149\n",
      " -2.61801 -2.54019 -2.54142 -2.69429 -2.57979 -2.61771 -2.58065 -2.84854\n",
      " -2.73536 -2.73444 -2.7735  -2.69553 -2.81064 -2.77134 -2.57887 -2.77304\n",
      " -2.77281 -2.77011 -2.61709 -2.77334 -2.50352 -2.65422 -2.50406 -2.73305\n",
      " -2.61677 -2.57964 -2.77235 -2.61793]\n",
      "ankleGyro1: [-9.17400e-03 -1.85884e-02  2.31511e-02 -9.12181e-03  1.37973e-02\n",
      "  4.33173e-03 -2.96489e-03 -2.58889e-02 -3.69895e-02 -2.77112e-02\n",
      "  4.17027e-02  5.57460e-02 -2.84203e-03  1.89407e-02  8.07114e-02\n",
      " -7.63168e-03  3.89628e-02  2.30023e-02 -2.50354e-02  1.63669e-02\n",
      " -3.68718e-02 -6.06022e-03 -3.81098e-02 -7.55444e-03 -4.32548e-02\n",
      "  1.20481e-02  1.59698e-02 -1.29792e-02 -3.10161e-04  7.54698e-02\n",
      " -7.36106e-03  1.10297e-03 -6.67484e-03  2.91787e-02  7.05979e-02\n",
      "  4.06400e-02 -2.42104e-02  7.76976e-03  1.67043e-02 -3.00288e-02\n",
      "  3.42701e-02  8.23369e-03  3.51211e-02  3.90035e-02  3.01970e-02\n",
      "  2.37894e-02  3.93695e-02 -1.81334e-03  2.87381e-02  2.37669e-02\n",
      "  3.30422e-03  2.34455e-02  5.76426e-02 -1.81181e-03  1.27822e-03\n",
      "  3.66049e-02 -8.87586e-03  8.50959e-03  1.55226e-02  3.52559e-02\n",
      "  3.56127e-02  2.26142e-02  2.43369e-02  2.92087e-02  4.41695e-02\n",
      "  5.03826e-02  3.66311e-02  2.25095e-03  2.38781e-02  1.34128e-02\n",
      " -1.92157e-02 -5.28134e-03  1.96362e-02 -2.58888e-02 -6.08769e-03\n",
      "  2.38153e-02  3.30923e-02  3.13708e-02  3.47561e-03 -2.29937e-02\n",
      "  2.72363e-02  1.89370e-02 -2.54614e-02  4.65315e-02  5.45733e-02\n",
      "  8.27687e-03  3.64032e-03 -7.46829e-04 -1.29466e-02  1.30168e-02\n",
      "  3.23655e-02  8.73708e-03 -1.30395e-02  2.24936e-02  2.21225e-02\n",
      "  7.94721e-05 -3.83778e-02  2.08324e-03  2.82588e-03  2.25303e-02]\n",
      "ankleGyro2: [-0.0589759   0.0200636   0.0208822  -0.00307502 -0.00987205 -0.00917265\n",
      " -0.0250477   0.0302304   0.00371889 -0.0251198   0.0138528  -0.00328188\n",
      " -0.0210158  -0.0302018   0.010543   -0.00891644 -0.0170844   0.012427\n",
      " -0.0219015   0.0204839   0.059121    0.0361852  -0.00439488 -0.0347079\n",
      " -0.0586238  -0.00852753  0.00395605 -0.0457286   0.0330988   0.00375984\n",
      "  0.0252875  -0.00339556 -0.0317507  -0.0319222  -0.0140893   0.00665808\n",
      " -0.0249107  -0.0106228  -0.0232499  -0.0184451  -0.00537647 -0.0206445\n",
      " -0.00612922  0.00766484 -0.0270464   0.0353435   0.0190292  -0.0146596\n",
      " -0.0189308  -0.0407313   0.0126978   0.00016688  0.0298795  -0.044322\n",
      "  0.00081516  0.011173    0.00137716  0.012855    0.0451473  -0.0302793\n",
      "  0.0078313  -0.0265132   0.0292195   0.0492356  -0.0444226   0.014852\n",
      "  0.0112624  -0.0248547  -0.0128514  -0.0191321  -0.00158503  0.0105759\n",
      " -0.0104875  -0.024047    0.0093138   0.0397668   0.0369789   0.00947282\n",
      " -0.0106047  -0.00996521  0.011481   -0.00777149  0.0157497   0.00405452\n",
      " -0.0115745   0.00628066 -0.00570745 -0.0320621  -0.0166723  -0.00598861\n",
      " -0.0378424  -0.0334212   0.00834313 -0.0262022   0.0130814  -0.00901152\n",
      " -0.0393122   0.0288504  -0.00338489 -0.0253547 ]\n",
      "ankleGyro3: [ 0.00142731 -0.00888457 -0.0197954  -0.0130153   0.0326701  -0.00673521\n",
      "  0.0033373   0.00458311 -0.00357211 -0.00933231 -0.0079964   0.0296913\n",
      "  0.00101222 -0.0234365  -0.0331324  -0.0217313  -0.0216853   0.0218751\n",
      " -0.00378092 -0.00484994  0.00955157  0.0188704  -0.00601437 -0.00306728\n",
      " -0.00338552  0.002744    0.0406828   0.0109944  -0.0197426  -0.00327576\n",
      " -0.0143202  -0.00905275 -0.0258816   0.00020648 -0.0160598  -0.00097703\n",
      "  0.0198252   0.0192556   0.0089201  -0.00139843 -0.00740856 -0.00289384\n",
      "  0.000464    0.0108442   0.00340583  0.0386346   0.0109517  -0.00837832\n",
      " -0.00203606 -0.00540455 -0.0159857   0.0170348   0.0221299   0.0267917\n",
      "  0.00437875  0.0258378   0.0216875   0.0139636   0.0174358   0.0207191\n",
      "  0.0147755   0.0262353   0.0023426  -0.0084852   0.0198222   0.0353614\n",
      "  0.0337161   0.0364379   0.0169139   0.00343588  0.0145429  -0.0152404\n",
      " -0.00276858  0.0190411   0.0138919  -0.00071081 -0.00228802  0.00296366\n",
      "  0.00742839 -0.0194184   0.0234528   0.0193117   0.0186277   0.00295275\n",
      "  0.0114925   0.00681164  0.00983797 -0.0345341  -0.00606328  0.0145864\n",
      " -0.0100813   0.0182566  -0.0215837  -0.0178755   0.00533638 -0.00595716\n",
      " -0.00476705 -0.0284412  -0.00274621 -0.0147165 ]\n",
      "Subject: 8\n",
      "Activity: 1\n",
      "\n",
      "\n",
      "Middle Window:\n",
      "handAcc16_1: [ -6.02398   -5.98807   -5.74672   -5.64806   -6.46696   -7.76365\n",
      "  -6.87024   -4.82772   -2.94555   -2.99486   -5.401     -5.828\n",
      "  -6.11829   -7.60964   -6.15488   -2.65652   -3.86697   -4.71524\n",
      "  -6.22428  -10.1381   -11.3631    -9.61773   -7.73877   -4.20842\n",
      "  -1.50694   -0.922794  -1.65651   -5.30689   -9.1093   -12.1304\n",
      " -13.2659    -8.78496   -4.61929   -2.98167   -3.45253   -8.39831\n",
      " -11.3617   -10.1273    -8.07708   -5.94209   -3.69997   -4.65126\n",
      "  -8.20139  -10.3038    -9.25119   -7.39545   -5.09254   -2.90441\n",
      "  -3.15429   -6.62179   -9.87019  -12.0812   -12.7093   -11.9884\n",
      "  -8.54891   -6.80222   -6.56015   -5.89348   -5.17416   -6.38718\n",
      "  -7.94984   -9.69058  -11.6613   -11.1945    -7.22681   -5.64135\n",
      "  -5.65204   -6.47425   -7.81161   -9.62729  -10.4635   -10.41\n",
      "  -9.68511   -7.79535   -6.9742    -9.06762  -10.3363   -11.6271\n",
      " -12.1048   -10.3489    -7.94836   -6.84039   -7.29559   -7.80218\n",
      "  -8.94968   -9.8474    -9.5582   -11.3127   -11.4883   -11.9434\n",
      " -10.9221    -7.4311    -7.03343   -8.07702   -9.35037  -11.1176\n",
      " -11.3854    -9.48018   -8.49651   -7.8918  ]\n",
      "handAcc16_2: [ 3.93951    4.43109    5.04873    4.8548     4.61758    4.52068\n",
      "  4.60557    4.2873     4.43214    5.45209    5.08538    3.93004\n",
      "  4.50348    4.98447    3.46319    4.37883    5.79892    4.59895\n",
      "  5.993      6.44403    6.10856    4.19353    4.45117    1.31273\n",
      "  2.58429    2.44254    4.31943    6.32091    6.75103    6.9447\n",
      "  4.03683    1.15767    1.2414    -0.0653971  5.36903    9.35691\n",
      "  8.46679    5.64165    2.88194    0.40861    1.60645    4.34868\n",
      "  6.95196    6.53562    4.92566    3.01876    0.520368   1.24164\n",
      "  4.46651    6.49311    6.23393    5.79156    4.87164    3.25664\n",
      "  2.11362    3.22014    3.99922    4.46458    3.89985    3.57595\n",
      "  3.7263     3.96635    4.55795    3.60244    3.15514    2.97143\n",
      "  2.59002    3.27067    4.03464    3.35672    1.97042    0.658312\n",
      " -0.585518  -1.13025    0.459962   1.19182    1.91559    1.84829\n",
      " -0.662424  -2.47953   -2.1521    -0.122201   1.5949     1.7561\n",
      "  1.06992   -1.46798   -1.8059    -1.55951   -1.12836   -0.405688\n",
      " -2.06872   -1.17446   -0.452846   0.0415338 -0.959655  -2.15563\n",
      " -2.00171   -1.48213   -1.19623   -0.869796 ]\n",
      "handAcc16_3: [ 5.24783    5.09276    6.47983    6.09709    4.31521    5.06586\n",
      "  6.84937    6.9965     7.87093    7.32793    6.63884    4.36559\n",
      "  4.59023    6.14425    7.01789    6.41292    7.15975    7.99781\n",
      "  5.58429    5.90901    5.50692    5.03919    5.79773    5.86119\n",
      "  6.16678    7.79248    9.12222    9.09927    7.19347    4.99227\n",
      "  5.52389    5.71619    6.00923    5.57632    6.12824    6.46407\n",
      "  6.15327    5.29608    3.98905    5.06853    5.21353    5.53642\n",
      "  5.01261    3.28908    2.38658    2.92115    5.58121    7.53592\n",
      "  7.29038    6.00001    3.8739     3.38043    1.79614    0.0419676\n",
      "  2.52205    4.73826    7.20245    8.90431    5.79951    2.47252\n",
      "  0.9476     0.805253   1.58197    5.44093    5.57884    3.40943\n",
      "  3.17963    2.20284    1.60292    0.577334   1.80108    3.61522\n",
      "  5.16976    5.31537    1.43508    0.823944   0.0327508 -0.140306\n",
      "  2.94       4.93529    5.47048    4.3256     5.00576    5.61343\n",
      "  4.2515     3.93871    0.36486    0.91506    2.29652    2.09483\n",
      "  7.92733    4.9361     5.36299    4.57594    1.82764    2.30554\n",
      "  6.53462    6.90779    5.80543    4.65876  ]\n",
      "handGyro1: [-0.142409   -0.125265   -0.115667   -0.116573   -0.227461   -0.308254\n",
      " -0.247754   -0.105275   -0.0605869   0.029665    0.00476397 -0.159823\n",
      " -0.24341    -0.239673   -0.223379   -0.27467    -0.253393   -0.183246\n",
      " -0.334655   -0.413745   -0.457676   -0.402582   -0.430036   -0.168871\n",
      " -0.184202   -0.0450335  -0.0349923  -0.159145   -0.402342   -0.658729\n",
      " -0.750958   -0.736315   -0.477141   -0.267899   -0.235805   -0.116453\n",
      " -0.164538   -0.624591   -0.937084   -0.772271   -0.515243   -0.167401\n",
      "  0.0379387  -0.0928562  -0.367804   -0.546143   -0.513427   -0.366456\n",
      "  0.0233224   0.326332    0.195326   -0.421587   -0.802062   -0.816335\n",
      " -0.624192   -0.379175   -0.0655646   0.0774991   0.00867826 -0.327001\n",
      " -0.663418   -0.949376   -1.18487    -1.33225    -1.26441    -1.15773\n",
      " -1.2232     -1.37227    -1.59733    -1.84883    -2.08982    -2.08399\n",
      " -1.77678    -1.26774    -0.775271   -0.461672   -0.28835    -0.312602\n",
      " -0.440602   -0.300049    0.073283    0.615118    0.851155    0.770181\n",
      "  0.42171     0.113753   -0.0901416  -0.106902    0.0707488   0.0915042\n",
      "  0.0684098   0.248099    0.546225    0.586308    0.350642   -0.0334451\n",
      " -0.0406205   0.281276    0.325078    0.225067  ]\n",
      "handGyro2: [-0.571046  -0.537513  -0.550222  -0.583621  -0.593137  -0.581012\n",
      " -0.556339  -0.585273  -0.621602  -0.638481  -0.684529  -0.671177\n",
      " -0.649272  -0.569451  -0.559528  -0.578173  -0.602226  -0.557045\n",
      " -0.579847  -0.556441  -0.571947  -0.527768  -0.4773    -0.453015\n",
      " -0.433287  -0.414631  -0.415724  -0.460406  -0.495256  -0.453496\n",
      " -0.397072  -0.251903  -0.190915  -0.0551383  0.103738   0.129446\n",
      "  0.21058    0.240579   0.319645   0.382203   0.431258   0.452475\n",
      "  0.50305    0.612391   0.704458   0.823222   0.918503   0.89173\n",
      "  0.793538   0.749447   0.77612    0.810074   0.881951   1.10666\n",
      "  1.30366    1.36772    1.47597    1.41598    1.29244    1.35831\n",
      "  1.42257    1.5147     1.61313    1.6903     1.60358    1.61453\n",
      "  1.63061    1.65564    1.64489    1.68623    1.6386     1.58879\n",
      "  1.41925    1.31051    1.15749    1.22141    1.15658    1.15673\n",
      "  1.11896    1.03345    0.879494   0.6362     0.576933   0.440452\n",
      "  0.330305   0.281483   0.169602   0.232915   0.203228   0.153523\n",
      "  0.0871936 -0.207942  -0.351229  -0.449507  -0.451039  -0.425856\n",
      " -0.482828  -0.697137  -0.857827  -0.96458  ]\n",
      "handGyro3: [ 0.892416   0.884531   0.872548   0.934963   0.843799   0.879901\n",
      "  0.946664   0.988536   0.947002   0.974375   0.928277   0.881436\n",
      "  0.850134   0.891197   0.936853   0.900486   0.894138   0.850555\n",
      "  0.758432   0.748128   0.789669   0.899607   0.867454   0.883347\n",
      "  0.700788   0.561991   0.336476   0.192196   0.0859518  0.0363223\n",
      "  0.108801   0.177184   0.220975   0.0693272 -0.131517  -0.0948692\n",
      " -0.153877  -0.196519  -0.252361  -0.302503  -0.415382  -0.444414\n",
      " -0.440099  -0.424908  -0.458115  -0.404803  -0.517448  -0.640897\n",
      " -0.720029  -0.783507  -0.853712  -0.985177  -0.89591   -0.776037\n",
      " -0.760483  -0.717613  -0.722557  -0.835431  -1.02331   -1.29798\n",
      " -1.3897    -1.41386   -1.39371   -1.32211   -1.17613   -1.1209\n",
      " -1.25243   -1.22014   -1.22343   -1.23123   -1.16739   -1.0979\n",
      " -1.04421   -0.947485  -0.898364  -0.873821  -0.849418  -0.768677\n",
      " -0.782431  -0.729042  -0.668137  -0.64886   -0.644349  -0.689085\n",
      " -0.759862  -0.78635   -0.740817  -0.738993  -0.596415  -0.562653\n",
      " -0.527494  -0.488717  -0.417587  -0.398511  -0.454281  -0.507785\n",
      " -0.447048  -0.388763  -0.420953  -0.377241 ]\n",
      "chestAcc16_1: [ 0.740245   0.930419   1.1917     1.25162    1.25246    1.25172\n",
      "  1.3374     1.23031    1.04412    0.815031   0.44184    0.481098\n",
      "  0.673313   0.82075    0.841908   1.03134    1.06885    1.15393\n",
      "  1.23369    1.1692     1.3972     1.61783    1.57987    1.79642\n",
      "  1.54715    1.2794     1.01667    0.944374   0.75783    0.678967\n",
      "  0.819043   0.846465   1.00807    0.860737   0.653581   0.444338\n",
      "  0.264903   0.276341   0.343719   0.333226   0.645424   0.651689\n",
      "  0.858621   0.84547    1.04266    1.33405    1.46726    1.59779\n",
      "  1.2467     0.787983   0.59731    0.351318   0.410989   0.438013\n",
      "  0.557677   0.795909   1.26049    1.34413    1.24143    0.861025\n",
      "  0.386991   0.08093   -0.485668  -0.900852  -1.08233   -0.957641\n",
      " -0.829125   0.0208342  0.061336  -0.283138  -0.128691   0.144277\n",
      "  0.135174   0.290466   0.153323   0.345387   0.229254   0.140543\n",
      " -0.050676  -0.167754  -0.274737  -0.454916  -0.701259  -0.785992\n",
      " -0.886312  -0.905756  -0.889245  -0.860281  -0.816149  -0.74559\n",
      " -0.707029  -0.552929  -0.431424  -0.390177  -0.205669  -0.399574\n",
      " -0.635969  -0.79529   -0.908689  -0.908291 ]\n",
      "chestAcc16_2: [4.51679 4.36558 4.43876 4.32193 4.20923 4.35959 4.47438 4.62691 4.55323\n",
      " 4.40333 3.95476 3.9549  4.14283 4.40463 4.47593 4.47508 4.24904 4.13791\n",
      " 4.1761  4.29189 4.89311 5.30451 5.79387 6.3925  6.38855 6.46443 6.27803\n",
      " 6.46731 6.69461 6.9953  7.06728 6.91421 6.91581 6.69166 6.62199 6.62711\n",
      " 6.51734 6.51993 6.63058 6.55294 6.13094 5.90658 5.60864 5.53915 5.87873\n",
      " 5.94998 6.0204  5.9397  5.48737 5.41324 5.37618 5.74928 5.97108 5.6674\n",
      " 5.51772 4.80419 4.50335 4.279   4.35726 4.20809 4.3939  4.39459 4.17175\n",
      " 3.98583 3.53692 3.12497 2.8644  2.86438 2.90243 2.79028 2.67736 2.41451\n",
      " 1.99851 1.77289 1.51342 1.21212 1.40022 1.43526 1.62387 1.88702 2.0772\n",
      " 2.11778 1.92634 1.7365  1.85292 1.84852 2.04041 2.03822 2.15236 1.7369\n",
      " 1.47346 1.66151 1.77567 1.66337 1.96245 1.99992 2.11178 2.1107  2.03601\n",
      " 2.18662]\n",
      "chestAcc16_3: [ -8.70991  -8.70597  -8.89508  -9.43571  -9.3966   -9.43583  -9.12488\n",
      "  -8.89499  -8.74324  -8.7469   -8.44244  -8.40301  -8.36141  -8.5532\n",
      "  -9.13386  -9.16915  -9.16773  -8.85606  -8.73858  -8.35292  -8.42818\n",
      "  -8.73529  -8.77629  -9.23896  -9.94044  -9.98426  -9.83353  -9.71929\n",
      "  -9.60725  -9.72582 -10.1494  -10.4969  -10.1842   -9.99258  -9.26041\n",
      "  -8.60596  -8.22165  -7.83424  -8.14315  -8.49157  -9.49129  -9.25812\n",
      "  -8.67259  -7.82074  -7.62467  -8.08427  -8.74034  -9.47342  -9.71063\n",
      "  -9.75742  -9.76076 -10.386   -10.9664  -11.3134  -11.1172  -10.7233\n",
      " -10.4429  -10.1696   -9.78449  -9.75218 -10.3422  -10.3865  -10.2024\n",
      " -10.0544   -9.70776  -9.31696  -8.81041  -8.4078   -8.32975  -8.37435\n",
      "  -8.29374  -8.05561  -8.32547  -8.20576  -7.66531  -7.58341  -7.66357\n",
      "  -8.09122  -8.13402  -8.25316  -8.02339  -7.6783   -8.26295  -8.53491\n",
      "  -8.07246  -8.73105  -8.18929  -8.49852  -8.30449  -8.45674  -8.41647\n",
      "  -8.37556  -8.14141  -8.02413  -8.25409  -8.37389  -8.64958  -8.88479\n",
      "  -8.84788  -8.84836]\n",
      "chestGyro1: [-0.21494    -0.450316   -0.579097   -0.573178   -0.48131    -0.462617\n",
      " -0.504316   -0.484527   -0.447123   -0.442062   -0.479389   -0.538508\n",
      " -0.4581     -0.321107   -0.253096   -0.274753   -0.37003    -0.34093\n",
      " -0.275725   -0.169321   -0.149181   -0.0556683  -0.131909   -0.197624\n",
      " -0.230981   -0.265109   -0.20363    -0.119589    0.0347729   0.207587\n",
      "  0.357164    0.351394    0.327021    0.322934    0.329785    0.392082\n",
      "  0.470831    0.673007    0.750881    0.744225    0.71822     0.682848\n",
      "  0.616879    0.473196    0.498035    0.538658    0.548291    0.472962\n",
      "  0.389245    0.478998    0.643345    0.836479    0.907029    0.771815\n",
      "  0.635336    0.436471    0.408098    0.458982    0.686509    0.839692\n",
      "  0.86021     0.905777    0.861031    0.853404    0.834436    0.872898\n",
      "  0.873766    0.875381    0.840678    0.852757    0.861727    0.849455\n",
      "  0.831447    0.783997    0.780988    0.739843    0.74473     0.756094\n",
      "  0.675738    0.579327    0.370991    0.226043    0.0750379   0.00450319\n",
      "  0.0647771   0.0383244  -0.0320653  -0.210481   -0.351909   -0.522906\n",
      " -0.602268   -0.599226   -0.653317   -0.622472   -0.672315   -0.641649\n",
      " -0.642952   -0.664886   -0.685356   -0.748001  ]\n",
      "chestGyro2: [ 0.15942    0.0901119  0.142554   0.191088   0.258947   0.253292\n",
      "  0.299092   0.365634   0.43441    0.426428   0.369073   0.346537\n",
      "  0.380309   0.296216   0.241512   0.177787   0.156682   0.10785\n",
      "  0.159591   0.173744   0.148591   0.17136    0.232414   0.275229\n",
      "  0.24936    0.206668   0.216884   0.289591   0.250577   0.236551\n",
      "  0.223367   0.168597   0.191192   0.185319   0.172349   0.200683\n",
      "  0.20579    0.13467    0.0187727  0.065988   0.0523144  0.0966606\n",
      "  0.0984603  0.158498   0.144612   0.163616   0.131451   0.0850808\n",
      "  0.0749172  0.167822   0.233397   0.314101   0.319281   0.319578\n",
      "  0.265649   0.303859   0.285599   0.244862   0.219055   0.21398\n",
      "  0.27246    0.219357   0.230726   0.184279   0.0987684  0.0425343\n",
      " -0.0328261 -0.072411  -0.173902  -0.137375  -0.155547  -0.178655\n",
      " -0.178127  -0.193847  -0.175574  -0.17883   -0.123226  -0.0938307\n",
      " -0.119994  -0.140014  -0.0960385 -0.108746  -0.165396  -0.165891\n",
      " -0.192488  -0.311324  -0.292351  -0.357461  -0.366246  -0.362679\n",
      " -0.274949  -0.313014  -0.283212  -0.32483   -0.348948  -0.328289\n",
      " -0.334854  -0.297545  -0.336228  -0.3028   ]\n",
      "chestGyro3: [-0.0835282 -0.0858277 -0.0612994 -0.0676157 -0.0860468 -0.0879682\n",
      " -0.0522246 -0.0906771 -0.118028  -0.16134   -0.131587  -0.154537\n",
      " -0.188707  -0.22616   -0.19516   -0.206585  -0.154745  -0.167383\n",
      " -0.200004  -0.199687  -0.209004  -0.192832  -0.215723  -0.229181\n",
      " -0.28224   -0.317435  -0.362548  -0.413285  -0.483266  -0.568096\n",
      " -0.586737  -0.559433  -0.511723  -0.50645   -0.546711  -0.595158\n",
      " -0.630741  -0.656357  -0.706252  -0.657973  -0.671427  -0.712242\n",
      " -0.724822  -0.676154  -0.641554  -0.644149  -0.600693  -0.588029\n",
      " -0.547084  -0.584705  -0.604147  -0.532902  -0.503036  -0.423683\n",
      " -0.443466  -0.48195   -0.50969   -0.619871  -0.756361  -0.874708\n",
      " -0.91888   -0.957437  -0.922149  -0.900315  -0.857082  -0.764083\n",
      " -0.628425  -0.514686  -0.386637  -0.335433  -0.294347  -0.289018\n",
      " -0.278279  -0.241875  -0.252082  -0.256718  -0.250016  -0.243952\n",
      " -0.219486  -0.21356   -0.150132  -0.145668  -0.0864272 -0.0371613\n",
      "  0.0180244  0.0483386  0.106451   0.208904   0.2558     0.332379\n",
      "  0.359848   0.374919   0.427301   0.421743   0.431755   0.393039\n",
      "  0.39318    0.408322   0.378174   0.38056  ]\n",
      "ankleAcc16_1: [10.5342  10.4674  10.2495  10.2031  10.203    9.6753  10.1126  11.1154\n",
      " 11.2005  11.3145  11.6129  10.563    9.89865 10.4005  11.079   10.8594\n",
      " 10.9984  11.0947  10.8049  10.5354  11.2905  11.6874  12.1732  12.6099\n",
      " 12.7976  11.9827  12.2674  12.6278  12.4442  11.7885  11.6127  12.0999\n",
      " 12.5176  11.6806  10.8312  10.5596  10.6954  10.5342  10.9944  10.9166\n",
      " 11.0819  10.9013  10.3608  11.1003  10.6925  10.1246  10.6764  11.5049\n",
      " 11.9303  11.9455  11.3605  10.2851  11.5099  11.8502  12.3077  12.7785\n",
      " 12.5033  11.6838  11.2002  11.4234  11.0235  10.4683   9.55485  9.93502\n",
      " 10.166   10.7702  10.2351   9.92612  9.9569   9.99229  9.5955   9.11171\n",
      "  9.81872 10.0355  10.3276  10.4873  10.0153   9.67729  9.5741   9.1997\n",
      "  9.89547 11.0144  11.1513  11.4266  11.4424  11.4575  10.9983  10.2678\n",
      "  9.34107  8.25744  7.1817   7.27317  8.81349 11.1274  11.7584  10.4055\n",
      "  7.9079   6.54325  6.62544  8.19141]\n",
      "ankleAcc16_2: [ 0.85515    0.092719  -0.326083   0.0570527 -1.6541     0.175563\n",
      "  1.92538    1.92122    3.66698    4.42629    1.91493   -0.587572\n",
      " -1.53669   -0.289384  -0.218999   1.37807    2.47234    2.59036\n",
      "  1.68751    0.246368   1.07629    1.83852    2.32999    2.47219\n",
      "  3.00321    1.67502    2.00898    2.72247    4.05352    2.0205\n",
      "  0.393936   1.98778    1.86934    1.37349    1.86997    2.77637\n",
      "  2.85553    2.55562    0.240631   1.1546     2.68083    2.45196\n",
      "  3.286      3.65348    2.51413    1.26473    3.01319    4.41345\n",
      "  5.77579    2.77796   -2.39386   -4.69881   -3.71495   -1.77876\n",
      "  1.93184    3.55779    2.49858    1.1444     0.652282   0.385079\n",
      "  0.128737  -0.592     -2.02687   -0.900866   1.61625    2.63761\n",
      "  2.18788    1.20387    0.140837  -0.885516  -0.773178   0.179762\n",
      "  1.50737    1.69836    1.35635    0.0976216 -0.702097  -1.34607\n",
      " -1.08183   -0.470997   1.22947    2.59244    2.67128    2.70373\n",
      "  2.77517    2.69474    1.82591    0.429113  -0.17599   -0.473826\n",
      "  1.16538    4.35427    6.73849    7.60028    4.9086     2.56115\n",
      "  1.31926    3.07137    5.95876    7.77579  ]\n",
      "ankleAcc16_3: [-1.37986   -0.951329  -0.642248  -1.031     -0.711663  -1.1915\n",
      " -1.93108   -2.1909    -2.16227   -2.24299   -1.79986   -1.4092\n",
      " -0.90846   -0.602338  -0.441459  -0.56939    0.158446  -0.420142\n",
      " -1.22789   -1.22171   -1.14194   -1.79862   -1.91246   -1.05989\n",
      " -1.13844   -0.598303   0.135745   0.829626   0.703715  -0.911155\n",
      " -1.5203    -1.79532   -1.55876   -0.63812   -0.341195   0.460776\n",
      " -0.039987  -0.387085  -0.869664  -1.14626   -2.19596   -1.92626\n",
      " -1.35814   -0.426683  -0.0764874 -0.0745578 -0.735773  -0.813145\n",
      " -0.5857    -0.798477   0.655433   0.118455  -0.415344  -0.655407\n",
      "  0.252406   0.710219   0.443843  -0.482307  -0.291264  -0.287317\n",
      " -1.1388    -0.792713  -1.0633     0.245654  -1.31118   -1.31132\n",
      " -1.62274   -1.77416   -1.80583   -1.68333   -1.18643   -1.27449\n",
      " -1.66137   -1.96905   -2.15688   -1.56859   -0.912444  -0.834734\n",
      " -0.567306  -0.690704  -0.231047  -0.613927  -1.07609   -0.61039\n",
      " -0.070417   0.47049    0.278225  -0.145083   0.273654   0.380158\n",
      "  0.397543   0.378691   0.148154  -0.21936   -1.19951   -0.967283\n",
      " -0.406346  -0.045318  -0.332521  -0.867992 ]\n",
      "ankleGyro1: [-3.86135e-02 -6.55386e-03 -8.80019e-02 -9.75992e-02 -4.90729e-02\n",
      "  6.06610e-04  9.24179e-02  2.66581e-01  3.35094e-01  2.80472e-01\n",
      "  2.41679e-01  7.59056e-02 -4.15012e-02 -4.41034e-02  1.53328e-01\n",
      "  2.58943e-01  2.02293e-01  2.03355e-01  1.45999e-01  1.17446e-01\n",
      "  7.49067e-02  1.49783e-01  2.70285e-01  2.16752e-01  1.29487e-01\n",
      " -2.12489e-02 -1.84999e-01 -1.63954e-01 -1.01096e-01 -9.69513e-02\n",
      " -4.41670e-02  3.32196e-02  1.03790e-01 -1.01826e-02 -1.33580e-01\n",
      " -1.34091e-01 -1.40776e-02  4.30135e-02 -2.59793e-01 -1.12773e-01\n",
      "  1.06927e-01  2.74071e-01  1.41088e-01 -1.15928e-02 -6.52149e-02\n",
      " -2.56218e-01  2.91157e-02  2.31753e-01  3.89057e-01  2.68868e-01\n",
      "  1.01127e-02 -3.28344e-01 -2.21000e-01  1.87870e-01  4.20489e-01\n",
      "  4.50951e-01  4.32303e-01  3.75623e-01  3.30899e-01  4.24839e-01\n",
      "  4.60192e-01  5.30750e-01  4.18907e-01  2.88422e-01  4.67277e-01\n",
      "  6.70998e-01  8.81366e-01  7.75208e-01  7.02235e-01  7.52710e-01\n",
      "  6.73295e-01  5.61143e-01  5.48534e-01  5.57021e-01  5.26176e-01\n",
      "  4.46191e-01  3.00123e-01  1.24510e-01 -4.73007e-02 -2.51073e-02\n",
      "  7.05100e-02  2.33977e-01  4.43382e-01  4.04366e-01  2.30416e-01\n",
      "  5.47519e-02 -4.51673e-02 -5.53675e-02 -4.97329e-02  1.90529e-02\n",
      " -1.11149e-02  1.97429e-01  6.40048e-01  6.32692e-01  4.20941e-01\n",
      "  2.88233e-01  7.37687e-02 -1.80043e-01 -7.68463e-02  5.05446e-02]\n",
      "ankleGyro2: [ 0.072097    0.0135481   0.027423   -0.00560751 -0.00438167  0.0810685\n",
      "  0.0202927   0.0398513   0.0272963   0.0301434  -0.00274661 -0.0222987\n",
      " -0.0476712  -0.0549729  -0.00629446  0.0409191   0.0239292   0.0817898\n",
      "  0.102939    0.0350545   0.00910785  0.0367872  -0.00196246  0.00094569\n",
      " -0.00765385 -0.029433   -0.0378336   0.0380663   0.17148     0.173119\n",
      "  0.114146    0.106863    0.0520205   0.106239    0.129463    0.142213\n",
      "  0.190843    0.190508    0.229322    0.20893     0.207366    0.211997\n",
      "  0.179064    0.196345    0.254672    0.322445    0.239298    0.272917\n",
      "  0.16697     0.173462    0.15357     0.153469    0.0393237   0.0933699\n",
      "  0.044822    0.0510814   0.147507    0.086976    0.079717    0.0687892\n",
      "  0.118298    0.0127447   0.0307051   0.0474365   0.0827826   0.0534525\n",
      "  0.0638779   0.0828621   0.0492621   0.0374962   0.035054    0.0534038\n",
      " -0.0107718   0.00257619  0.00945729  0.0427299   0.083712    0.104856\n",
      "  0.0591894   0.112186    0.070723    0.0246027  -0.0275488  -0.0416489\n",
      " -0.0785885  -0.0807846  -0.040053   -0.0628135  -0.108244   -0.073537\n",
      " -0.0411781  -0.0462806  -0.176104   -0.206679   -0.142173   -0.0721128\n",
      " -0.0586791  -0.0713158  -0.0409379  -0.0217648 ]\n",
      "ankleGyro3: [ 0.143584    0.0982184   0.0618982   0.0568368   0.0317683   0.0563326\n",
      "  0.0741223  -0.0150381  -0.0456974  -0.109621   -0.193532   -0.148463\n",
      " -0.105684   -0.0419527  -0.0701229  -0.0166237   0.0256057   0.0434814\n",
      "  0.00808056  0.0223358   0.0361598  -0.00907236 -0.0359017  -0.0202606\n",
      " -0.0505049  -0.0818277  -0.0500402  -0.097325   -0.171531   -0.228222\n",
      " -0.17507    -0.143425   -0.170537   -0.120701   -0.0396708  -0.0631847\n",
      " -0.0537477  -0.0465109  -0.0162221   0.0816532   0.0640987   0.0685208\n",
      "  0.111975    0.150335    0.10899     0.179232    0.196533    0.0760703\n",
      " -0.0269152  -0.119336   -0.110751    0.0994714   0.188486    0.15462\n",
      "  0.119329   -0.019174   -0.171928   -0.241966   -0.305958   -0.398767\n",
      " -0.506129   -0.562075   -0.549597   -0.489916   -0.604911   -0.826004\n",
      " -0.960844   -1.0539     -1.16469    -1.25368    -1.28936    -1.38559\n",
      " -1.49414    -1.64997    -1.7839     -1.82476    -1.82476    -1.80454\n",
      " -1.78809    -1.77415    -1.83179    -1.94207    -2.00976    -2.08562\n",
      " -2.0234     -2.05461    -2.00328    -2.03582    -1.94092    -1.81752\n",
      " -1.71174    -1.71833    -1.87908    -2.01724    -2.01326    -1.84516\n",
      " -1.62661    -1.46591    -1.47086    -1.52707   ]\n",
      "Subject: 4\n",
      "Activity: 6\n",
      "\n",
      "\n",
      "Last Window:\n",
      "handAcc16_1: [-3.23745 -3.2468  -3.60241 -3.78975 -3.68145 -3.91273 -3.7195  -3.99312\n",
      " -4.02797 -3.80444 -3.58331 -3.32333 -2.93261 -2.63591 -2.59411 -2.44108\n",
      " -2.36256 -2.16907 -2.20446 -2.40169 -2.47219 -2.5475  -2.70214 -2.85731\n",
      " -3.01248 -3.00606 -3.00874 -3.09341 -3.0894  -3.46543 -3.51257 -3.62407\n",
      " -3.78886 -3.57897 -3.68404 -3.69983 -3.87835 -3.84003 -4.11472 -3.896\n",
      " -3.6009  -3.34573 -3.27951 -3.10634 -2.97336 -2.99235 -2.63987 -2.38107\n",
      " -2.08945 -1.93856 -1.93267 -1.64239 -1.63463 -1.72064 -1.42662 -1.35826\n",
      " -1.43652 -1.36496 -1.45125 -1.50028 -1.49734 -1.60563 -1.64208 -1.61873\n",
      " -1.48335 -1.66748 -1.49084 -1.56536 -1.57258 -1.94673 -2.10109 -2.32569\n",
      " -2.48112 -2.41115 -2.26454 -2.45242 -2.41998 -2.53976 -2.68316 -2.90241\n",
      " -3.1246  -3.31407 -3.23475 -3.27307 -3.20177 -3.20952 -2.90186 -2.90935\n",
      " -2.71907 -2.64162 -2.86274 -2.92976 -2.96193 -2.69634 -2.77218 -2.61862\n",
      " -2.43315 -2.40659 -2.27959 -2.28735]\n",
      "handAcc16_2: [ 8.70689  8.51518  8.21094  7.87195  7.33959  7.0751   7.11258  7.15022\n",
      "  6.99976  6.92146  6.88043  6.8015   7.10478  7.25253  7.4426   7.51765\n",
      "  7.55567  7.6311   7.78348  7.55563  7.70898  7.7475   7.82354  7.8237\n",
      "  7.82386  7.7876   7.78693  7.8231   8.01347  8.0926   8.09054  7.97827\n",
      "  7.55943  7.43983  7.48066  7.70393  7.8595   8.31381  8.19968  8.00656\n",
      "  7.54795  7.54356  7.35126  7.38372  7.26437  7.03238  7.59117  8.92299\n",
      "  8.91798  8.87887  8.57737  8.57201  8.53609  8.57193  8.90836  9.01958\n",
      "  9.20886  9.58599 10.152   10.4524  10.6804  10.6447  10.6453  10.4899\n",
      " 10.3333  10.4496  10.5965  10.9382  11.05    10.8266  10.9406  10.6778\n",
      " 10.4507  10.2214  10.3327  10.2966  10.295   10.1427   9.91862  9.84651\n",
      "  9.73578  9.472    9.24513  9.16954  9.13001  8.97658  9.16494  9.23881\n",
      "  9.42705  9.69204  9.73307  9.62218  9.47239  9.43275  9.39539  9.35696\n",
      "  9.61973  9.73024  9.96002 10.1853 ]\n",
      "handAcc16_3: [ 6.19718   5.96676   5.57759   5.69139   5.65632   5.57677   5.695\n",
      "  5.42137   5.53681   5.46345   5.31295   5.20165   5.51438   5.32589\n",
      "  5.40285   5.44337   5.5214    5.6395    5.67695   5.48232   5.59621\n",
      "  5.59495   5.47692   5.35913   5.24135   5.434     5.35699   5.08619\n",
      "  5.16258   5.19517   4.92506   5.00074   4.69178   4.23348   4.4627\n",
      "  3.96137   4.22758   4.14965   3.87651   3.688     3.5785    3.31293\n",
      "  3.08365   2.62428   2.12634   1.6265    0.783176  1.43684   1.13333\n",
      "  1.13572   1.36775   1.02573   1.25689   0.947586  0.642956  0.41268\n",
      "  0.295408  0.10277  -0.323793 -0.710415 -0.672652 -0.520204 -0.482265\n",
      " -0.866265 -1.28707  -1.17476  -1.55748  -1.59824  -1.82965  -1.68055\n",
      " -1.79871  -1.68572  -1.76426  -1.8779   -2.03003  -1.99423  -2.14769\n",
      " -2.30292  -2.0349   -1.80702  -1.65602  -1.61949  -1.46358  -1.4639\n",
      " -1.57816  -1.7702   -1.65076  -1.88205  -1.87983  -1.87957  -1.72907\n",
      " -1.46029  -1.26783  -1.22523  -1.22624  -1.14683  -1.29889  -1.64525\n",
      " -1.29774  -1.56803 ]\n",
      "handGyro1: [ 0.1453     0.0281365 -0.0758518 -0.209505  -0.195292  -0.138682\n",
      " -0.0109095  0.142339   0.263133   0.340531   0.448669   0.534967\n",
      "  0.577264   0.606292   0.601954   0.53067    0.508776   0.480403\n",
      "  0.501018   0.497014   0.56141    0.612968   0.709553   0.724412\n",
      "  0.863764   0.927266   0.983362   1.03295    0.992465   1.03136\n",
      "  0.956203   0.918988   0.853472   0.942223   1.05375    1.18887\n",
      "  1.33506    1.4549     1.50522    1.56051    1.66638    1.79679\n",
      "  1.97884    2.15479    2.24747    2.32451    2.46043    2.37126\n",
      "  2.28533    2.16668    2.08221    2.12341    2.13351    2.24256\n",
      "  2.33878    2.38852    2.39796    2.41685    2.44083    2.41905\n",
      "  2.3888     2.28934    2.25287    2.2317     2.16186    2.11837\n",
      "  2.17034    2.11177    2.01683    1.95662    1.87204    1.71867\n",
      "  1.58391    1.45575    1.28913    1.11004    0.934319   0.701757\n",
      "  0.524642   0.409208   0.331044   0.322484   0.352495   0.337112\n",
      "  0.364113   0.440997   0.483929   0.516704   0.481447   0.382649\n",
      "  0.318124   0.220543   0.0927591  0.0231523  0.014755  -0.0292288\n",
      " -0.123197  -0.153313  -0.165935  -0.202328 ]\n",
      "handGyro2: [ 0.0606077   0.00246155 -0.0281982  -0.090083   -0.0882927  -0.142633\n",
      " -0.191226   -0.26119    -0.339463   -0.374361   -0.45138    -0.494234\n",
      " -0.487825   -0.522949   -0.493433   -0.504161   -0.476472   -0.532878\n",
      " -0.46559    -0.413906   -0.455968   -0.43827    -0.47538    -0.502918\n",
      " -0.566644   -0.633084   -0.651488   -0.667519   -0.691522   -0.717932\n",
      " -0.688434   -0.720103   -0.817102   -0.892091   -0.830979   -0.904321\n",
      " -0.977579   -0.995368   -1.03794    -1.09129    -1.1559     -1.20791\n",
      " -1.24147    -1.28711    -1.34471    -1.32947    -1.36618    -1.48574\n",
      " -1.38442    -1.38695    -1.38574    -1.34945    -1.31165    -1.30685\n",
      " -1.28047    -1.25891    -1.2004     -1.21641    -1.16882    -1.12199\n",
      " -1.1326     -1.12874    -1.08394    -1.02512    -0.991238   -0.923728\n",
      " -0.870193   -0.795421   -0.74432    -0.725848   -0.660663   -0.592814\n",
      " -0.569386   -0.536559   -0.495915   -0.374793   -0.343593   -0.275217\n",
      " -0.235136   -0.158046   -0.132275   -0.101338   -0.0506219  -0.0668003\n",
      " -0.0354134   0.0151031  -0.0217662   0.0393448   0.0670752   0.104821\n",
      "  0.190567    0.231147    0.266415    0.288947    0.364662    0.388444\n",
      "  0.448644    0.496185    0.542344    0.571052  ]\n",
      "handGyro3: [ 2.99956e-02  2.82595e-02  1.42133e-02  8.79875e-03  4.02859e-03\n",
      " -1.16426e-03  4.78503e-03  3.79519e-02  3.57877e-03  2.26818e-02\n",
      " -7.86639e-02 -8.05914e-02 -1.17434e-01 -1.65818e-01 -2.14222e-01\n",
      " -2.53937e-01 -2.96780e-01 -2.92287e-01 -2.91516e-01 -2.94914e-01\n",
      " -2.84568e-01 -2.22324e-01 -2.04713e-01 -1.82012e-01 -1.26264e-01\n",
      " -5.98958e-02 -6.56065e-02 -4.20796e-02 -3.75341e-02 -4.95770e-02\n",
      " -5.37843e-02 -1.16291e-01 -1.34483e-01 -1.50786e-01 -1.59055e-01\n",
      " -2.05616e-01 -2.26126e-01 -2.20121e-01 -2.49249e-01 -3.07905e-01\n",
      " -3.52895e-01 -3.51199e-01 -4.40149e-01 -5.08679e-01 -5.63642e-01\n",
      " -6.35677e-01 -7.78809e-01 -6.91082e-01 -6.32113e-01 -6.78031e-01\n",
      " -7.30902e-01 -7.75150e-01 -8.05551e-01 -8.52436e-01 -8.97122e-01\n",
      " -9.04968e-01 -8.82066e-01 -8.98212e-01 -8.86335e-01 -8.72574e-01\n",
      " -8.13984e-01 -7.92598e-01 -7.52354e-01 -6.98925e-01 -6.09335e-01\n",
      " -5.30013e-01 -4.72456e-01 -3.93097e-01 -2.69854e-01 -2.29100e-01\n",
      " -2.07210e-01 -1.96161e-01 -1.70425e-01 -1.11712e-01 -4.82348e-02\n",
      "  1.29618e-04  3.05946e-02  5.06545e-02  1.00607e-01  1.19135e-01\n",
      "  1.03070e-01  1.68298e-01  1.68656e-01  1.98014e-01  1.96873e-01\n",
      "  2.20398e-01  2.20604e-01  2.02115e-01  1.99207e-01  2.05820e-01\n",
      "  1.72651e-01  1.60152e-01  1.26198e-01  1.38550e-01  1.30022e-01\n",
      "  1.10996e-01  1.10041e-01  1.24552e-01  1.38411e-01  1.11288e-01]\n",
      "chestAcc16_1: [-0.394525 -0.506182 -0.543352 -0.507625 -0.547728 -0.550016 -0.583655\n",
      " -0.619283 -0.627787 -0.473986 -0.625201 -0.743574 -0.618139 -0.66257\n",
      " -0.810852 -0.745662 -0.660083 -0.780246 -0.545242 -0.590717 -0.400889\n",
      " -0.470654 -0.283313 -0.316307 -0.273318 -0.355067 -0.358199 -0.621124\n",
      " -0.584898 -0.620925 -0.652177 -0.694468 -0.733925 -0.618338 -0.61615\n",
      " -0.574405 -0.538032 -0.426422 -0.399446 -0.705061 -0.736611 -0.615704\n",
      " -0.688004 -0.693523 -0.822091 -0.626843 -0.624356 -0.663515 -0.626544\n",
      " -0.656851 -0.584898 -0.622814 -0.660928 -0.700187 -0.85553  -0.732583\n",
      " -0.7386   -0.770996 -0.821891 -0.773184 -0.930118 -0.929074 -0.739644\n",
      " -0.965845 -1.00396  -1.03447  -1.03854  -1.00157  -1.00222  -1.00013\n",
      " -1.07253  -0.993267 -0.994312 -0.915795 -1.00023  -0.925343 -0.844639\n",
      " -0.918282 -0.880466 -0.732284 -0.803192 -0.687506 -0.724476 -0.68845\n",
      " -0.759259 -0.878278 -0.913061 -0.839219 -1.06905  -1.12139  -1.11691\n",
      " -1.03193  -0.793297 -0.795385 -0.761846 -0.535992 -0.690538 -0.460707\n",
      " -0.496833 -0.418117]\n",
      "chestAcc16_2: [ 9.70139  9.85288  9.77795  9.73966  9.85222  9.85171  9.81521  9.89115\n",
      "  9.70107  9.77616  9.81455  9.58821  9.89141  9.66431  9.51521  9.663\n",
      "  9.74014  9.70155  9.92805  9.73835  9.8881   9.73928  9.81322  9.58871\n",
      "  9.70259  9.77684  9.88902  9.62732  9.7773   9.70262  9.7043   9.854\n",
      "  9.77855  9.8161   9.77897  9.85493  9.62864  9.89107 10.0013   9.7387\n",
      "  9.62742  9.51565  9.70494  9.77895  9.58793  9.62602  9.70185  9.73936\n",
      "  9.73898  9.66561  9.7773   9.85272  9.85284  9.8527   9.62674  9.85411\n",
      "  9.73986  9.74127  9.66324  9.77841  9.8155   9.77811  9.77725  9.85379\n",
      "  9.85391  9.70523  9.89246  9.89208 10.0801  10.0053  10.1569  10.0069\n",
      " 10.0443  10.0445   9.96765  9.89184  9.85499  9.96871  9.85563  9.96707\n",
      "  9.81799  9.8932   9.89358  9.96825  9.85682  9.81849  9.78173  9.74332\n",
      "  9.74378  9.55252  9.51591  9.36713  9.66971  9.7445   9.74335  9.96777\n",
      " 10.043   10.0426   9.93026 10.0058 ]\n",
      "chestAcc16_3: [ 1.19779e-01  1.94706e-01  2.32997e-01  1.56327e-01  7.77927e-02\n",
      "  3.11088e-04  1.54704e-01  2.31254e-01 -3.93292e-02  1.93405e-03\n",
      "  3.77907e-02 -7.97810e-02  2.69994e-01  7.63230e-02  2.29005e-01\n",
      " -1.57504e-01  1.53563e-01 -4.20928e-02  1.55033e-01 -7.74997e-02\n",
      " -1.13268e-01  1.18277e-01  5.26794e-03  1.99005e-01  3.54297e-01\n",
      "  1.58970e-01  4.23858e-02  1.93357e-01  1.16084e-01  1.93116e-01\n",
      "  4.24870e-01  2.68733e-01  2.29543e-01  2.70236e-01  3.47838e-01\n",
      "  4.64510e-01  4.27184e-01  3.12190e-01 -7.48893e-02 -7.95724e-02\n",
      "  1.52543e-01  3.87423e-01  5.01661e-01  3.07715e-01 -1.58644e-01\n",
      " -3.47193e-04  7.68933e-02  3.73410e-02 -7.08949e-04  2.70027e-01\n",
      "  1.16084e-01  1.15152e-01  1.14461e-01  7.50291e-02 -4.49257e-03\n",
      "  2.68042e-01  7.47000e-02  2.67713e-01 -1.58886e-01  1.90111e-01\n",
      "  3.22635e-02  7.11249e-02  3.58386e-02  1.08934e-01  1.08243e-01\n",
      "  3.79220e-01  2.23654e-01  1.85604e-01  1.46260e-01  2.23983e-01\n",
      "  3.38341e-01  4.56428e-01  4.17566e-01  4.96430e-01  2.24104e-01\n",
      "  1.86986e-01  3.43451e-01  4.19189e-01  4.20242e-01  2.67681e-01\n",
      "  4.60485e-01  5.01058e-01  5.39108e-01  4.62076e-01  6.54760e-01\n",
      "  4.97844e-01  6.13496e-01  5.37517e-01  4.94631e-01  2.96532e-02\n",
      "  1.84737e-01  4.96528e-01  8.09635e-01  7.31912e-01  5.77640e-01\n",
      "  4.64839e-01  3.84353e-01  4.27239e-01  5.04392e-01  5.83014e-01]\n",
      "chestGyro1: [-0.00053776 -0.0344101  -0.0655206  -0.0825634  -0.0150064  -0.0118202\n",
      "  0.0204195  -0.00692316 -0.0587573  -0.0439225  -0.0193347  -0.0289967\n",
      " -0.0593698  -0.0569281  -0.073235   -0.00763367 -0.0348712  -0.0533551\n",
      " -0.0365032  -0.0650404  -0.041487   -0.0252615  -0.00648712 -0.0304398\n",
      " -0.0409469  -0.0663306  -0.0663794  -0.0427487  -0.0853419  -0.0609786\n",
      " -0.029669   -0.0428378  -0.038872   -0.0570121  -0.0882869  -0.0829947\n",
      " -0.0689494  -0.090485   -0.0552249  -0.00955801  0.011837   -0.00036371\n",
      " -0.0513423  -0.0786945  -0.044576   -0.0510248  -0.053528   -0.00585426\n",
      " -0.0050753  -0.0313477  -0.0821274  -0.0565222  -0.0561212  -0.0555598\n",
      " -0.0879008  -0.0331344  -0.0393613  -0.0131423  -0.0193434   0.0041494\n",
      " -0.0277187  -0.00675114  0.00562658  0.0195073   0.0484025   0.0424303\n",
      "  0.0393518   0.00541052  0.0055223   0.0465122   0.0308863   0.00016836\n",
      "  0.010361   -0.00266993 -0.0118745  -0.0248236  -0.016712   -0.0323419\n",
      " -0.00451219 -0.0087522  -0.0683304  -0.058712   -0.0665795  -0.0641952\n",
      " -0.0819933  -0.0477228  -0.102056   -0.11355    -0.0760352  -0.105381\n",
      " -0.00255306 -0.0101017  -0.0646003  -0.116412   -0.0902036  -0.094148\n",
      " -0.0847817  -0.0894509  -0.128185   -0.12046   ]\n",
      "chestGyro2: [ 0.00900889  0.0115636  -0.0139586  -0.015047   -0.0160719  -0.0254015\n",
      "  0.00197349 -0.00057194  0.0244569   0.0315972  -0.0119372   0.00294929\n",
      "  0.0119261  -0.0445785  -0.0659506  -0.0633082  -0.0599035  -0.065098\n",
      " -0.126074   -0.0474549  -0.00343447 -0.0638804  -0.0404007  -0.0488039\n",
      " -0.0259136  -0.0464305  -0.0384421  -0.0677009  -0.0421902  -0.0525306\n",
      " -0.0391172  -0.0525797  -0.0303122   0.0224042  -0.029353   -0.00305399\n",
      " -0.0515364  -0.0426917  -0.0188313  -0.00823075  0.0245857  -0.0275631\n",
      " -0.0826128  -0.0543846  -0.0264681  -0.0491485  -0.0584439  -0.0510208\n",
      " -0.0535726  -0.0631671  -0.0548758  -0.0603842  -0.0732521  -0.0982482\n",
      " -0.0746918  -0.0859985  -0.068522   -0.0800347  -0.122812   -0.128961\n",
      " -0.167964   -0.21504    -0.206632   -0.192385   -0.214771   -0.229614\n",
      " -0.262787   -0.261126   -0.235365   -0.275603   -0.219483   -0.195083\n",
      " -0.21519    -0.208345   -0.210907   -0.186311   -0.217583   -0.243243\n",
      " -0.142652   -0.169269   -0.149443   -0.0610636  -0.0334522  -0.0305609\n",
      " -0.0480342   0.0305446   0.0205742   0.0155644   0.0553192   0.119212\n",
      "  0.138287    0.15593     0.163158    0.210551    0.216313    0.216427\n",
      "  0.275158    0.276148    0.302362    0.359845  ]\n",
      "chestGyro3: [-0.0142224  -0.0212301   0.0151149   0.0249489  -0.029661   -0.0162513\n",
      " -0.0268738  -0.0256664  -0.0138882  -0.00250541  0.00308202 -0.00320568\n",
      " -0.00412098 -0.00673342 -0.00583589 -0.00775867 -0.0154514  -0.00546433\n",
      " -0.0257085  -0.0244669  -0.0463248  -0.054392   -0.0402032  -0.0336862\n",
      " -0.0318203  -0.0332178  -0.0243387   0.00152006 -0.00622848  0.0171114\n",
      "  0.00740765 -0.0138304   0.0156148   0.00369969  0.00536655  0.00317691\n",
      "  0.00904942  0.0144907   0.0140585  -0.0140808  -0.013093    0.00679549\n",
      "  0.00726778  0.024593    0.0258892   0.0136586  -0.00949144 -0.00714811\n",
      " -0.003188   -0.0158112   0.010412    0.00293241 -0.00759272  0.00933491\n",
      " -0.0188494  -0.0212546  -0.0327446  -0.0289756  -0.023444   -0.00405663\n",
      " -0.0217233  -0.0399845  -0.0477304  -0.0249516  -0.04648    -0.0434253\n",
      " -0.0277284  -0.0662216  -0.0807969  -0.0744819  -0.0567814  -0.0512178\n",
      " -0.0667824  -0.0799817  -0.0418833  -0.0429975  -0.0373395  -0.0284317\n",
      " -0.0152613  -0.0341756  -0.0110953   0.0314225   0.0418939   0.0238792\n",
      "  0.0677253   0.076218    0.0769868   0.0797926   0.06619     0.10064\n",
      "  0.100513    0.0818913   0.0834658   0.12025     0.121579    0.151057\n",
      "  0.164562    0.156967    0.154473    0.185914  ]\n",
      "ankleAcc16_1: [ 9.77651  9.70036  9.70075  9.69506  9.80984  9.7738   9.77081  9.80953\n",
      "  9.77501  9.73558  9.73279  9.76951  9.85297  9.7679   9.73818  9.88879\n",
      "  9.7683   9.81283  9.73598  9.85266  9.77651  9.92991  9.73217  9.77521\n",
      "  9.6272   9.74128  9.77761  9.7749   9.78201  9.78201  9.85325  9.81433\n",
      "  9.85565  9.73539  9.96144  9.81374  9.85517  9.85946  9.88788  9.88619\n",
      "  9.88929  9.85006  9.93011  9.85616  9.92692  9.88848  9.92551  9.89268\n",
      "  9.93022  9.89739 10.008   10.0465  10.1566  10.2269  10.0808  10.3433\n",
      " 10.351   10.4655  10.7243  10.4658  10.6545  10.7634  10.7221  10.7206\n",
      " 10.5735  10.3855  10.4274  10.3915  10.4004   9.95979  9.44864  9.45852\n",
      "  9.6509   9.52793  9.38731  9.01325  8.77707  8.76308  8.78024  8.29575\n",
      "  7.92901  7.48078  7.16936  6.89789  6.83417  6.40928  7.00987  7.21884\n",
      "  7.21439  7.86436  7.49553  6.85727  7.15545  6.75292  7.32113  7.73673\n",
      "  8.34391  8.25578  8.75585  9.11023]\n",
      "ankleAcc16_2: [1.00397  0.9668   1.04274  1.00637  0.852825 1.11884  0.967611 1.00502\n",
      " 0.928357 0.967205 0.853942 0.929963 0.888951 1.04451  1.0425   1.00326\n",
      " 1.12045  1.00406  1.04314  1.04115  1.00397  1.078    1.15833  0.966326\n",
      " 1.08086  1.00357  1.00365  1.11852  1.00237  1.00237  1.15505  1.07967\n",
      " 1.19238  0.929237 1.00343  0.965767 0.888308 1.07719  1.04155  0.927967\n",
      " 0.889035 0.965852 1.11597  1.07815  0.926766 1.15546  1.07928  1.1162\n",
      " 0.925803 0.962723 0.848427 0.847869 0.8478   1.03878  0.886564 0.770678\n",
      " 0.768429 0.767075 0.576215 0.61488  0.499146 0.689562 0.995154 1.14767\n",
      " 1.22391  1.26339  1.49     1.56585  1.37312  1.52559  1.44912  1.86452\n",
      " 2.66036  2.51209  2.77657  2.81691  3.12593  2.97791  2.26109  1.81795\n",
      " 1.79079  2.17364  2.4072   2.06939  1.88718  2.32262  4.13288  5.15203\n",
      " 5.98991  7.35082  2.27746  0.837073 4.39932  3.83955  3.45374  4.61812\n",
      " 4.94343  4.29064  3.71321  3.05951 ]\n",
      "ankleAcc16_3: [-4.23816e-01 -4.62956e-01 -4.63423e-01 -6.56203e-01 -5.38305e-01\n",
      " -5.40325e-01 -6.16596e-01 -5.77842e-01 -4.61951e-01 -5.39776e-01\n",
      " -6.16281e-01 -6.54966e-01 -3.45139e-01 -7.32872e-01 -4.63037e-01\n",
      " -4.22660e-01 -7.33339e-01 -4.62033e-01 -5.40243e-01 -3.84676e-01\n",
      " -4.23816e-01 -3.06933e-01 -6.95355e-01 -4.62185e-01 -4.25824e-01\n",
      " -3.46995e-01 -3.85213e-01 -5.01722e-01 -2.30801e-01 -2.30801e-01\n",
      " -3.85377e-01 -4.23898e-01 -3.08405e-01 -5.39542e-01 -4.99095e-01\n",
      " -4.23197e-01 -2.67933e-01 -1.53292e-01 -4.61496e-01 -4.99398e-01\n",
      " -3.83356e-01 -4.61414e-01 -3.07167e-01 -2.69101e-01 -3.83205e-01\n",
      " -4.62197e-01 -4.61345e-01 -3.07552e-01 -2.67396e-01 -1.13603e-01\n",
      " -1.50350e-01 -1.11362e-01 -1.87411e-01 -3.80823e-01 -2.27018e-01\n",
      " -1.85018e-01  8.52021e-02  1.63564e-01  9.02229e-02  2.03101e-01\n",
      "  2.82934e-01  1.28510e-01 -6.67581e-02 -1.44898e-01 -6.97008e-02\n",
      " -1.10464e-01  4.32855e-03  4.20789e-02  3.90673e-01  6.55335e-01\n",
      "  1.11364e+00  1.38129e+00  1.41692e+00  1.06927e+00  1.33631e+00\n",
      "  1.33223e+00  8.64812e-01  4.02512e-01 -1.71707e-01 -1.33161e+00\n",
      " -2.37712e+00 -2.42269e+00 -2.89040e+00 -3.16122e+00 -4.04830e+00\n",
      " -5.90766e+00 -4.87082e+00 -4.29615e+00 -4.61012e+00 -4.38036e+00\n",
      " -5.85765e+00 -5.66231e+00 -5.06393e+00 -5.95214e+00 -5.63520e+00\n",
      " -4.40330e+00 -2.85551e+00 -1.88762e+00 -1.30007e+00 -5.59174e-01]\n",
      "ankleGyro1: [ 7.33290e-03  4.41237e-02 -1.93544e-02 -2.55687e-02  1.46100e-02\n",
      " -3.35696e-03 -8.23592e-02  1.55317e-02 -1.88027e-02  7.79957e-03\n",
      " -6.89736e-02  9.92786e-03 -2.58014e-02 -5.32824e-03 -3.38040e-02\n",
      "  8.11160e-04  1.62951e-03 -2.21297e-02  5.19980e-02  8.08330e-03\n",
      "  7.55380e-03 -8.44213e-03  5.97991e-03 -3.48761e-02  1.08217e-02\n",
      " -2.49663e-02 -4.11767e-02 -4.60588e-04 -1.86918e-02 -8.60603e-04\n",
      "  4.37758e-02 -2.47129e-02  2.82895e-02  4.78019e-02 -6.73987e-02\n",
      "  2.62175e-02  1.62855e-02  7.84906e-03  8.06082e-03  2.65848e-02\n",
      "  4.11416e-05  2.55704e-02  6.02982e-02  5.49124e-02  3.44762e-02\n",
      "  5.76777e-03  3.85243e-02  6.16808e-02  6.22181e-02  8.66062e-02\n",
      "  6.97864e-02  6.96073e-02  1.18268e-01  1.48487e-01  1.19592e-01\n",
      "  1.45444e-01  1.96671e-01  2.21642e-01  3.30977e-01  3.77266e-01\n",
      "  5.25084e-01  5.96633e-01  6.29156e-01  6.75829e-01  6.89801e-01\n",
      "  6.74340e-01  6.76710e-01  7.11850e-01  8.10193e-01  9.97479e-01\n",
      "  1.14210e+00  1.35197e+00  1.53951e+00  1.76218e+00  1.93345e+00\n",
      "  2.04605e+00  2.15334e+00  2.32840e+00  2.65592e+00  3.04260e+00\n",
      "  3.45780e+00  3.85889e+00  3.94945e+00  3.97751e+00  3.90336e+00\n",
      "  3.76542e+00  3.57408e+00  2.95144e+00  2.07062e+00  1.08350e+00\n",
      "  4.40116e-01  3.23111e-01  2.74333e-01 -6.74873e-02 -4.56207e-01\n",
      " -6.55259e-01 -8.61927e-01 -9.72910e-01 -1.08712e+00 -9.85939e-01]\n",
      "ankleGyro2: [ 0.0327712   0.034309   -0.00350363  0.0139384   0.0196346   0.0141645\n",
      "  0.0768027   0.020568    0.0128295   0.0734243   0.0154728   0.030656\n",
      "  0.00808632  0.00680343  0.0245718   0.0187733   0.0432417   0.0443265\n",
      "  0.0173853   0.00200734  0.0660829   0.0187525   0.018083    0.0462903\n",
      "  0.0626069   0.00944684  0.00988207  0.00364007 -0.00706719  0.04427\n",
      "  0.0266188   0.0204266  -0.00307697 -0.00614964  0.0410335   0.0138283\n",
      "  0.0238182   0.0250953   0.0315981   0.052697    0.0226925   0.0217794\n",
      "  0.0163655   0.010535    0.00760927  0.0173586   0.00458367  0.0130466\n",
      "  0.00255302 -0.00799004 -0.0240847  -0.00153127  0.0312665  -0.0334739\n",
      " -0.00192031 -0.0183942  -0.0355866  -0.0231884   0.0394317  -0.00088951\n",
      " -0.00704415 -0.0289273   0.00455612  0.00243491 -0.0106074  -0.0141629\n",
      " -0.0192141  -0.040607   -0.0301826  -0.0113     -0.0709038  -0.0317273\n",
      "  0.00027522  0.0883327   0.0931051   0.147897    0.146462    0.160346\n",
      "  0.119356    0.0900127   0.0199091   0.0166244  -0.0302732  -0.039999\n",
      " -0.048588   -0.0540377  -0.0511118   0.00032515  0.124295    0.211758\n",
      "  0.258483    0.104929    0.107948    0.0407771  -0.00549204 -0.0342824\n",
      " -0.00475778  0.00742052  0.00511255  0.0200231 ]\n",
      "ankleGyro3: [-2.44519e-02  1.18953e-02 -1.14708e-02  2.06795e-03 -1.35277e-02\n",
      "  2.02465e-02 -7.73256e-03 -1.24923e-04 -2.65107e-03 -2.09185e-02\n",
      "  3.92631e-04  1.17710e-02 -1.29546e-02  9.93169e-03 -1.75475e-02\n",
      " -6.08363e-02 -5.03641e-02 -2.99353e-02 -2.21155e-02 -1.21347e-02\n",
      " -2.41390e-02 -2.06862e-02 -3.40461e-02 -4.01910e-02 -3.36158e-02\n",
      " -3.34208e-02 -4.60632e-02 -3.81372e-02 -3.67101e-02 -3.93327e-02\n",
      " -2.20505e-02 -6.71899e-02 -3.81245e-02 -3.25878e-02 -3.32363e-02\n",
      " -5.84526e-02 -6.86257e-02 -5.44566e-02 -5.36073e-02 -6.91155e-02\n",
      " -7.57686e-02 -7.57100e-02 -8.51223e-02 -1.14337e-01 -7.89704e-02\n",
      " -9.78603e-02 -1.17587e-01 -1.57622e-01 -1.49841e-01 -1.42786e-01\n",
      " -1.81581e-01 -1.99488e-01 -2.34500e-01 -2.58660e-01 -2.89950e-01\n",
      " -2.99489e-01 -3.63322e-01 -4.08829e-01 -4.17407e-01 -4.58626e-01\n",
      " -5.33917e-01 -5.89076e-01 -6.14672e-01 -6.53956e-01 -6.84763e-01\n",
      " -6.88776e-01 -6.96694e-01 -6.81047e-01 -6.69663e-01 -6.35120e-01\n",
      " -6.33723e-01 -6.72190e-01 -6.97392e-01 -6.84951e-01 -7.26206e-01\n",
      " -6.98613e-01 -7.01495e-01 -7.15871e-01 -7.16969e-01 -7.22528e-01\n",
      " -7.21305e-01 -7.47851e-01 -7.28359e-01 -6.38574e-01 -5.56138e-01\n",
      " -4.69901e-01 -3.86876e-01 -3.25014e-01 -2.00131e-01 -1.89305e-01\n",
      " -1.77956e-01 -4.50009e-02  1.36867e-01  2.27502e-01  3.64688e-01\n",
      "  4.92295e-01  5.79467e-01  6.57275e-01  6.66406e-01  7.23529e-01]\n",
      "Subject: 3\n",
      "Activity: 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def display_window(window, subject, activity, feature_labels):\n",
    "    for i, feature_label in enumerate(feature_labels):\n",
    "        print(f\"{feature_label}: {window[:, i]}\")\n",
    "    print(f\"Subject: {subject}\")\n",
    "    print(f\"Activity: {activity}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Display first window\n",
    "print(\"First Window:\")\n",
    "display_window(windows[0], subjects[0], activities[0], sensor_data.columns[3:])\n",
    "\n",
    "# Display middle window\n",
    "middle_index = len(windows) // 2\n",
    "print(\"Middle Window:\")\n",
    "display_window(windows[middle_index], subjects[middle_index], activities[middle_index], sensor_data.columns[3:])\n",
    "\n",
    "# Display last window\n",
    "print(\"Last Window:\")\n",
    "display_window(windows[-1], subjects[-1], activities[-1], sensor_data.columns[3:])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Nested cross-validation method"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "#\n",
    "# skf = StratifiedKFold(n_splits=5)\n",
    "# activity_splits = skf.split(windows, activities)\n",
    "#\n",
    "# # Activity Stratification: Start by using StratifiedKFold from sklearn.model_selection. This will ensure that each fold has the same proportion of samples from each activity. You'll first split the dataset into 'k' folds maintaining the same distribution of activities in each fold."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GroupKFold\n",
    "#\n",
    "# subject_splits = []\n",
    "# for train_index, test_index in activity_splits:\n",
    "#     X_train, X_test = windows[train_index], windows[test_index]\n",
    "#     y_train, y_test = activities[train_index], activities[test_index]\n",
    "#     subjects_train, subjects_test = subjects[train_index], subjects[test_index]\n",
    "#\n",
    "#     gkf = GroupKFold(n_splits=5)\n",
    "#     subject_splits.append(gkf.split(X_train, y_train, subjects_train))\n",
    "#\n",
    "# # Subject Grouping: For each of the 'k' folds from the first level, you can further split them based on subjects. You can use GroupKFold for this purpose. This will ensure that all samples from the same subject stay in the same fold."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import LSTM, Dense\n",
    "#\n",
    "# # Create a list to store losses and accuracies\n",
    "# losses = []\n",
    "# accuracies = []\n",
    "#\n",
    "# num_features = windows.shape[2]\n",
    "# unique_activities = np.unique(activities)\n",
    "# num_classes = len(unique_activities)\n",
    "#\n",
    "# # For each subject split\n",
    "# for train_index, val_index in subject_splits:\n",
    "#     X_train, X_val = windows[train_index], windows[val_index]\n",
    "#     y_train, y_val = activities[train_index], activities[val_index]\n",
    "#\n",
    "#     # Build and compile your LSTM model\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(100, input_shape=(window_size, num_features)))\n",
    "#     model.add(Dense(num_classes, activation='softmax'))\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#\n",
    "#     # Train the model\n",
    "#     history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=64)\n",
    "#\n",
    "#     # Store the loss and accuracy\n",
    "#     losses.append(history.history['loss'])\n",
    "#     accuracies.append(history.history['accuracy'])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# # Calculate the average loss and accuracy\n",
    "# avg_loss = np.mean(losses, axis=0)\n",
    "# avg_accuracy = np.mean(accuracies, axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# import matplotlib.pyplot as plt\n",
    "#\n",
    "# num_features = windows.shape[2]\n",
    "# unique_activities = np.unique(activities)\n",
    "# unique_subjects = np.unique(subjects)\n",
    "# num_classes = len(unique_activities)\n",
    "#\n",
    "# epochs = 8\n",
    "# batch_size = 32\n",
    "# n_splits = 5\n",
    "#\n",
    "# # Re-label activities and subjects to start from zero\n",
    "# activity_map = {act: i for i, act in enumerate(unique_activities)}\n",
    "# subject_map = {sub: i for i, sub in enumerate(unique_subjects)}\n",
    "# activities_relabeled = np.array([activity_map[act] for act in activities])\n",
    "# subjects_relabeled = np.array([subject_map[sub] for sub in subjects])\n",
    "#\n",
    "# # Convert labels to categorical\n",
    "# activities_categorical = to_categorical(activities_relabeled)\n",
    "#\n",
    "# # Initialize the StratifiedKFold object\n",
    "# skf = StratifiedKFold(n_splits=n_splits)\n",
    "#\n",
    "# # Store all histories\n",
    "# all_histories = []\n",
    "#\n",
    "# # Loop over all activity splits\n",
    "# for activity_train_index, activity_test_index in skf.split(windows, activities_relabeled):\n",
    "#     # Split the data into training and testing sets\n",
    "#     X_train, X_test = windows[activity_train_index], windows[activity_test_index]\n",
    "#     y_train, y_test = activities_categorical[activity_train_index], activities_categorical[activity_test_index]\n",
    "#     subjects_train, subjects_test = subjects_relabeled[activity_train_index], subjects_relabeled[activity_test_index]\n",
    "#\n",
    "#     # Initialize the GroupKFold object\n",
    "#     gkf = GroupKFold(n_splits=n_splits)\n",
    "#\n",
    "#     # Loop over all subject splits\n",
    "#     for subject_train_index, subject_val_index in gkf.split(X_train, y_train, subjects_train):\n",
    "#         # Split the data into training and validation sets\n",
    "#         X_train_subject, X_val = X_train[subject_train_index], X_train[subject_val_index]\n",
    "#         y_train_subject, y_val = y_train[subject_train_index], y_train[subject_val_index]\n",
    "#\n",
    "#         # Define LSTM model\n",
    "#         model = Sequential()\n",
    "#         model.add(LSTM(100, input_shape=(window_size, num_features)))\n",
    "#         model.add(Dense(num_classes, activation='softmax'))\n",
    "#         model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#\n",
    "#         # Fit the model\n",
    "#         history = model.fit(X_train_subject, y_train_subject, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size)\n",
    "#\n",
    "#         # Append the history to the list of all histories\n",
    "#         all_histories.append(history)\n",
    "#\n",
    "# # Compute the average loss and accuracy over all histories\n",
    "# avg_loss = np.mean([history.history['loss'] for history in all_histories], axis=0)\n",
    "# avg_val_loss = np.mean([history.history['val_loss'] for history in all_histories], axis=0)\n",
    "# avg_accuracy = np.mean([history.history['accuracy'] for history in all_histories], axis=0)\n",
    "# avg_val_accuracy = np.mean([history.history['val_accuracy'] for history in all_histories], axis=0)\n",
    "#\n",
    "# # Plot the average loss and accuracy\n",
    "# plt.figure(figsize=(12, 6))\n",
    "#\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(avg_loss, label='Training Loss')\n",
    "# plt.plot(avg_val_loss, label='Validation Loss')\n",
    "# plt.title('Average Loss Over Folds')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "#\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(avg_accuracy, label='Training Accuracy')\n",
    "# plt.plot(avg_val_accuracy, label='Validation Accuracy')\n",
    "# plt.title('Average Accuracy Over Folds')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import LSTM, Dense\n",
    "# from keras.utils import to_categorical\n",
    "#\n",
    "# # Re-label activities and subjects to start from zero\n",
    "# unique_activities = np.unique(activities)\n",
    "# unique_subjects = np.unique(subjects)\n",
    "# activity_map = {act: i for i, act in enumerate(unique_activities)}\n",
    "# subject_map = {sub: i for i, sub in enumerate(unique_subjects)}\n",
    "# activities_relabeled = np.array([activity_map[act] for act in activities])\n",
    "# subjects_relabeled = np.array([subject_map[sub] for sub in subjects])\n",
    "#\n",
    "# num_features = windows.shape[2]\n",
    "# num_classes = len(unique_activities)\n",
    "#\n",
    "# epochs = 8\n",
    "# batch_size = 32\n",
    "#\n",
    "# # Convert labels to categorical\n",
    "# activities_categorical = to_categorical(activities_relabeled)\n",
    "#\n",
    "# # Split the data into training and testing sets\n",
    "# train_size = int(0.8 * len(windows))\n",
    "# X_train, X_test = windows[:train_size], windows[train_size:]\n",
    "# y_train, y_test = activities_categorical[:train_size], activities_categorical[train_size:]\n",
    "# subjects_train, subjects_test = subjects_relabeled[:train_size], subjects_relabeled[train_size:]\n",
    "#\n",
    "# # Define LSTM model\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(100, input_shape=(window_size, num_features)))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#\n",
    "# # Fit the model\n",
    "# history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size)\n",
    "#\n",
    "# # Plot the loss and accuracy curves\n",
    "# plt.figure(figsize=(12, 6))\n",
    "#\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(history.history['loss'], label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.title('Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "#\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "# plt.title('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "#\n",
    "# # Evaluate the model on the test set\n",
    "# loss, accuracy = model.evaluate(X_test, y_test)\n",
    "# print('Test Loss:', loss)\n",
    "# print('Test Accuracy:', accuracy)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# This looks like it is not an optimal approach\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Re-label activities and subjects to start from zero\n",
    "unique_activities = np.unique(activities)\n",
    "unique_subjects = np.unique(subjects)\n",
    "activity_map = {act: i for i, act in enumerate(unique_activities)}\n",
    "subject_map = {sub: i for i, sub in enumerate(unique_subjects)}\n",
    "\n",
    "activities_relabeled = np.array([activity_map[act] for act in activities])\n",
    "subjects_relabeled = np.array([subject_map[sub] for sub in subjects])\n",
    "\n",
    "# Create a combined label for stratification\n",
    "combined_label = subjects_relabeled * len(unique_activities) + activities_relabeled\n",
    "\n",
    "# StratifiedGroupKFold\n",
    "n_splits = 5\n",
    "sgkf = StratifiedGroupKFold(n_splits=n_splits)\n",
    "\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "for train_index, val_index in sgkf.split(windows, combined_label, groups=subjects_relabeled):\n",
    "    train_indices.append(train_index)\n",
    "    val_indices.append(val_index)\n",
    "\n",
    "# The `StratifiedGroupKFold` in the code above attempts to ensure that all activities are represented in both the training and test sets, while each subject group appears in just one of the splits. However, depending on the distribution of activities among the subjects, it might not always be possible to have every activity represented in every split.\n",
    "\n",
    "# The stratification in `StratifiedGroupKFold` is done based on the `y` parameter, which is `combined_label` in this case. This ensures that the distribution of activities (which is part of the `combined_label`) is similar in each fold.\n",
    "\n",
    "# The `groups` parameter is set as `subjects_relabeled`. This ensures that all samples with the same group (i.e., subject) are included in the same test set. So, the subjects do not overlap between training and test sets.\n",
    "\n",
    "# However, this setup does not guarantee that every activity from each subject is represented in both training and test sets. If a certain activity is performed by only one subject and that subject's data is placed in the test set, then the training set will not have any samples of that activity.\n",
    "\n",
    "# So while this method attempts to balance the distribution of activities and subjects, it's not always perfect, especially when the activities are not evenly distributed among the subjects. For a more controlled split, you might need to implement a custom split logic."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 18)\n",
      "12\n",
      "[ 1  2  3  4  5  6  7 12 13 16 17 24]\n",
      "Training fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 14:29:46.325218: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-05-16 14:29:46.325421: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Max\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 14:29:46.847492: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-05-16 14:29:47.458407: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-16 14:29:47.549747: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-16 14:29:47.863250: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "931/931 [==============================] - ETA: 0s - loss: 0.6497 - accuracy: 0.8202"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 14:30:17.019353: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-16 14:30:17.067219: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "931/931 [==============================] - 36s 37ms/step - loss: 0.6497 - accuracy: 0.8202 - val_loss: 0.7192 - val_accuracy: 0.8029\n",
      "Epoch 2/8\n",
      "931/931 [==============================] - 33s 35ms/step - loss: 0.3577 - accuracy: 0.9019 - val_loss: 0.7587 - val_accuracy: 0.7940\n",
      "Epoch 3/8\n",
      "931/931 [==============================] - 33s 36ms/step - loss: 0.3050 - accuracy: 0.9144 - val_loss: 0.7119 - val_accuracy: 0.8112\n",
      "Epoch 4/8\n",
      "931/931 [==============================] - 34s 36ms/step - loss: 0.2701 - accuracy: 0.9217 - val_loss: 0.7232 - val_accuracy: 0.8074\n",
      "Epoch 5/8\n",
      "931/931 [==============================] - 34s 36ms/step - loss: 0.2432 - accuracy: 0.9284 - val_loss: 0.7526 - val_accuracy: 0.8090\n",
      "Epoch 6/8\n",
      "255/931 [=======>......................] - ETA: 20s - loss: 0.2245 - accuracy: 0.9353"
     ]
    }
   ],
   "source": [
    "# # This looks like it is not an optimal approach\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, roc_curve, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define LSTM model\n",
    "def create_lstm_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=input_shape))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "input_shape = (window_size, windows.shape[2])\n",
    "num_classes = len(unique_activities)\n",
    "\n",
    "print(input_shape)\n",
    "print(num_classes)\n",
    "print(unique_activities)\n",
    "\n",
    "epochs = 14\n",
    "batch_size = 32\n",
    "history_list = []\n",
    "confusion_matrices = []\n",
    "f1_scores = []\n",
    "\n",
    "for i in range(n_splits):\n",
    "    print(f\"Training fold {i + 1}...\")\n",
    "    train_data = windows[train_indices[i]]\n",
    "    train_labels = to_categorical(activities_relabeled[train_indices[i]], num_classes)\n",
    "    val_data = windows[val_indices[i]]\n",
    "    val_labels = to_categorical(activities_relabeled[val_indices[i]], num_classes)\n",
    "\n",
    "    model = create_lstm_model(input_shape, num_classes)\n",
    "    history = model.fit(train_data, train_labels, validation_data=(val_data, val_labels), epochs=epochs, batch_size=batch_size)\n",
    "    history_list.append(history.history)\n",
    "\n",
    "    # Confusion matrix\n",
    "    val_pred = np.argmax(model.predict(val_data), axis=-1)\n",
    "    cm = confusion_matrix(activities_relabeled[val_indices[i]], val_pred)\n",
    "    confusion_matrices.append(cm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # # This looks like it is not an optimal approach\n",
    "# # Averaged accuracy and loss\n",
    "# avg_train_acc = np.mean([h['accuracy'] for h in history_list], axis=0)\n",
    "# avg_val_acc = np.mean([h['val_accuracy'] for h in history_list], axis=0)\n",
    "# avg_train_loss = np.mean([h['loss'] for h in history_list], axis=0)\n",
    "# avg_val_loss = np.mean([h['val_loss'] for h in history_list], axis=0)\n",
    "#\n",
    "# # Plot averaged accuracy and loss\n",
    "# plt.figure(figsize=(10, 5))\n",
    "#\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(range(1, epochs + 1), avg_train_acc, label='Training Accuracy')\n",
    "# plt.plot(range(1, epochs + 1), avg_val_acc, label='Validation Accuracy')\n",
    "# plt.title('Model Accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "#\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(range(1, epochs + 1), avg_train_loss, label='Training Loss')\n",
    "# plt.plot(range(1, epochs + 1), avg_val_loss, label='Validation Loss')\n",
    "# plt.title('Model Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Normalize the averaged confusion matrix\n",
    "avg_cm = np.mean(confusion_matrices, axis=0)\n",
    "normalized_avg_cm = avg_cm / np.sum(avg_cm, axis=1)[:, np.newaxis]\n",
    "\n",
    "# Create a list of labels for the unique_activities\n",
    "labels = [activityID[i] for i in unique_activities]\n",
    "\n",
    "# Plot normalized averaged confusion matrix\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(normalized_avg_cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Normalized Averaged Confusion Matrix')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.grid(False)  # Remove grid lines\n",
    "\n",
    "num_classes = len(labels)  # Number of unique activities\n",
    "\n",
    "tick_marks = np.arange(num_classes)\n",
    "plt.xticks(tick_marks, labels, rotation=45, ha='right')  # Rotate x-axis labels by 45 degrees\n",
    "plt.yticks(tick_marks, labels, rotation=45, va='center', ha='centre', rotation_mode='anchor')  # Rotate y-axis labels by 45 degrees\n",
    "\n",
    "# Print normalised values in the confusion matrix\n",
    "for i in range(num_classes):\n",
    "    for j in range(num_classes):\n",
    "        plt.text(j, i, format(normalized_avg_cm[i, j], '.2f'), horizontalalignment=\"center\", color=\"white\" if normalized_avg_cm[i, j] > 0.5 else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    # Calculate average metrics across folds\n",
    "avg_precision = np.mean([h.history['val_precision'][-1] for h in history_list])\n",
    "avg_accuracy = np.mean([h.history['val_accuracy'][-1] for h in history_list])\n",
    "avg_recall = np.mean([h.history['val_recall'][-1] for h in history_list])\n",
    "\n",
    "# Calculate average F1-score\n",
    "avg_f1_score = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall)\n",
    "\n",
    "# Print average metrics\n",
    "print(\"Average Precision:\", avg_precision)\n",
    "print(\"Average Accuracy:\", avg_accuracy)\n",
    "print(\"Average F1-score:\", avg_f1_score)\n",
    "print(\"Average Recall:\", avg_recall)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "\n",
    "# Define the number of folds for stratified k-fold cross-validation\n",
    "k = 5\n",
    "\n",
    "# Step 1: Stratify based on activities\n",
    "skf = StratifiedKFold(n_splits=k)\n",
    "\n",
    "# Initialize stratified index lists\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "# Step 2: Within each activity stratum, further stratify based on subject labels\n",
    "for train_index, val_index in skf.split(windows, activities):\n",
    "    train_sub_windows, train_sub_activities = windows[train_index], activities[train_index]\n",
    "    train_sub_subjects = subjects[train_index]\n",
    "    val_sub_windows, val_sub_activities = windows[val_index], activities[val_index]\n",
    "    val_sub_subjects = subjects[val_index]\n",
    "\n",
    "    gkf = GroupKFold(n_splits=k)\n",
    "    train_sub_indices = []\n",
    "    val_sub_indices = []\n",
    "    for train_sub_index, val_sub_index in gkf.split(train_sub_windows, train_sub_activities, train_sub_subjects):\n",
    "        train_sub_indices.append(train_index[train_sub_index])\n",
    "        val_sub_indices.append(train_index[val_sub_index])\n",
    "\n",
    "    train_indices.append(np.concatenate(train_sub_indices))\n",
    "    val_indices.append(np.concatenate(val_sub_indices))\n",
    "\n",
    "# At this point, train_indices and val_indices are lists of arrays,\n",
    "# where each array corresponds to the indices for one fold. You can\n",
    "# now use these indices to perform the k-fold cross-validation.\n",
    "\n",
    "# Use StratifiedKFold to stratify based on activity labels.\n",
    "# This will ensure an even distribution of activities across folds.\n",
    "# Within each activity stratum, further stratify based on subject labels\n",
    "# using GroupKFold. This will ensure that the same subject doesn't appear\n",
    "# in both the training and test sets in the same fold.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "n_splits = 5\n",
    "# Combine subject and activity arrays to create a single group identifier\n",
    "groups = [f\"{subject}_{activity}\" for subject, activity in zip(subjects, activities)]\n",
    "\n",
    "gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "for train_index, val_index in gkf.split(windows, activities, groups):\n",
    "    train_indices.append(train_index)\n",
    "    val_indices.append(val_index)\n",
    "\n",
    "# At this point, train_indices and val_indices are lists of arrays,\n",
    "# where each array corresponds to the indices for one fold. You can\n",
    "# now use these indices to perform the k-fold cross-validation.\n",
    "# It will try to maintain an even distribution of subjects and activities\n",
    "# across the training and validation sets for each fold.\n",
    "# However, this method still has the same limitations as the previous method,\n",
    "# so perfect balance might not be possible in some cases."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Step 3: Stratify the data and apply k-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "X = np.array(windows)\n",
    "y_activity = np.array(activities)\n",
    "y_subject = np.array(subjects)\n",
    "\n",
    "for train_val_index, test_index in skf.split(X, y_activity):\n",
    "    X_train_val, X_test = X[train_val_index], X[test_index]\n",
    "    y_activity_train_val, y_activity_test = y_activity[train_val_index], y_activity[test_index]\n",
    "    y_subject_train_val, y_subject_test = y_subject[train_val_index], y_subject[test_index]\n",
    "\n",
    "    skf_inner = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_index, val_index in skf_inner.split(X_train_val, y_activity_train_val):\n",
    "        X_train, X_val = X_train_val[train_index], X_train_val[val_index]\n",
    "        y_activity_train, y_activity_val = y_activity_train_val[train_index], y_activity_train_val[val_index]\n",
    "        y_subject_train, y_subject_val = y_subject_train_val[train_index], y_subject_train_val[val_index]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Stratify the data and apply k-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "X = np.array(windows)\n",
    "y_activity = np.array(activities)\n",
    "y_subject = np.array(subjects)\n",
    "\n",
    "accuracies = []\n",
    "losses = []\n",
    "\n",
    "for train_val_index, test_index in tqdm(skf.split(X, y_activity), desc='Outer Loop'):\n",
    "    X_train_val, X_test = X[train_val_index], X[test_index]\n",
    "    y_activity_train_val, y_activity_test = y_activity[train_val_index], y_activity[test_index]\n",
    "    y_subject_train_val, y_subject_test = y_subject[train_val_index], y_subject[test_index]\n",
    "\n",
    "    skf_inner = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_index, val_index in tqdm(skf_inner.split(X_train_val, y_activity_train_val), desc='Inner Loop', leave=False):\n",
    "        X_train, X_val = X_train_val[train_index], X_train_val[val_index]\n",
    "        y_activity_train, y_activity_val = y_activity_train_val[train_index], y_activity_train_val[val_index]\n",
    "        y_subject_train, y_subject_val = y_subject_train_val[train_index], y_subject_train_val[val_index]\n",
    "\n",
    "        # Step 4: Train an LSTM model\n",
    "        input_shape = X_train.shape[1:]\n",
    "        num_classes = len(np.unique(y_activity))\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(128, input_shape=input_shape))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        history = model.fit(X_train, y_activity_train, epochs=10, batch_size=32, validation_data=(X_val, y_activity_val))\n",
    "\n",
    "        # Step 6: Evaluate the model on the testing set\n",
    "        loss, accuracy = model.evaluate(X_test, y_activity_test)\n",
    "        accuracies.append(accuracy)\n",
    "        losses.append(loss)\n",
    "\n",
    "        # Plot accuracy and loss graphs\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "# Step 7: Calculate average accuracy and loss\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "avg_loss = np.mean(losses)\n",
    "\n",
    "print(f\"Average Testing Accuracy: {avg_accuracy}\")\n",
    "print(f\"Average Testing Loss: {avg_loss}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Assuming train_data and test_data are your training and testing DataFrames\n",
    "# # Replace 'subjectID' and 'activityID' with the actual column names in your data\n",
    "\n",
    "# # For training data\n",
    "# train_grouped = train_data.groupby(['subjectID', 'activityID']).size().reset_index(name='count')\n",
    "# print(\"Training data samples count:\")\n",
    "# print(train_grouped)\n",
    "\n",
    "# # For testing data\n",
    "# test_grouped = test_data.groupby(['subjectID', 'activityID']).size().reset_index(name='count')\n",
    "# print(\"\\nTesting data samples count:\")\n",
    "# print(test_grouped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter the data for protocol activities\n",
    "protocol_acts = [1, 2, 3, 4, 5, 6, 7, 12, 13, 16, 17, 24]\n",
    "sensor_data = sensor_data[sensor_data['activityID'].isin(protocol_acts)]\n",
    "\n",
    "# Interpolate missing values\n",
    "sensor_data = sensor_data.interpolate()\n",
    "\n",
    "def create_windows(data, step_size, stride):\n",
    "    windows = []\n",
    "    for i in range(0, len(data) - step_size, stride):\n",
    "        windows.append(data.iloc[i:i+step_size])\n",
    "    return windows\n",
    "\n",
    "step_size = 100\n",
    "stride = 50\n",
    "\n",
    "# Stratify the data based on subjectID\n",
    "unique_subjects = sensor_data['subjectID'].unique()\n",
    "subject_windows = []\n",
    "\n",
    "for subject in tqdm(unique_subjects):\n",
    "    subject_data = sensor_data[sensor_data['subjectID'] == subject]\n",
    "    subject_data_windows = create_windows(subject_data, step_size, stride)\n",
    "    subject_windows.extend(subject_data_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_data and test_data are your training and testing DataFrames\n",
    "# Replace 'subjectID' and 'activityID' with the actual column names in your data\n",
    "\n",
    "# For training data\n",
    "train_grouped = train_data.groupby(['subjectID', 'activityID']).size().reset_index(name='count')\n",
    "print(\"Training data samples count:\")\n",
    "print(train_grouped)\n",
    "\n",
    "# For testing data\n",
    "test_grouped = test_data.groupby(['subjectID', 'activityID']).size().reset_index(name='count')\n",
    "print(\"\\nTesting data samples count:\")\n",
    "print(test_grouped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_curve, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data for LSTM model\n",
    "def prepare_data(windows):\n",
    "    activity_mapping = {activity: idx for idx, activity in enumerate(protocol_acts)}\n",
    "    x = [window.iloc[:, 3:].values.astype('float32') for window in windows]  # Drop subjectID and timestamp columns\n",
    "    y = [activity_mapping[window.iloc[0]['activityID']] for window in windows]\n",
    "    print(x)\n",
    "    print(y)\n",
    "    exit()\n",
    "    return np.array(x), to_categorical(y, num_classes=len(protocol_acts))\n",
    "\n",
    "\n",
    "X, y = prepare_data(subject_windows)\n",
    "\n",
    "# Apply k-fold cross-validation\n",
    "k = 5\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=input_shape))\n",
    "    model.add(Dense(len(protocol_acts), activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Train and validate the model\n",
    "# train_accuracies = []\n",
    "# train_losses = []\n",
    "# val_accuracies = []\n",
    "# val_losses = []\n",
    "# max_epochs_reached = 0\n",
    "#\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "#\n",
    "# for train_index, val_index in tqdm(skf.split(X, np.argmax(y, axis=1)), total=k, desc=\"Training progress\"):\n",
    "#     X_train, X_val = X[train_index], X[val_index]\n",
    "#     y_train, y_val = y[train_index], y[val_index]\n",
    "#\n",
    "#     model = create_lstm_model((X_train.shape[1], X_train.shape[2]))\n",
    "#\n",
    "#     history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "#\n",
    "#     train_accuracies.append(history.history['accuracy'])\n",
    "#     train_losses.append(history.history['loss'])\n",
    "#     val_accuracies.append(history.history['val_accuracy'])\n",
    "#     val_losses.append(history.history['val_loss'])\n",
    "#\n",
    "#     max_epochs_reached = max(max_epochs_reached, len(history.history['accuracy']))\n",
    "#\n",
    "# # Calculate average accuracy and loss\n",
    "# avg_train_accuracy = np.mean(train_accuracies, axis=0)\n",
    "# avg_train_loss = np.mean(train_losses, axis=0)\n",
    "# avg_val_accuracy = np.mean(val_accuracies, axis=0)\n",
    "# avg_val_loss = np.mean(val_losses, axis=0)\n",
    "\n",
    "\n",
    "train_accuracies = []\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "val_losses = []\n",
    "epochs_per_fold = []\n",
    "\n",
    "epochs = 30\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "for train_index, val_index in tqdm(skf.split(X, np.argmax(y, axis=1)), total=k, desc=\"Training progress\"):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    model = create_lstm_model((X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "    train_accuracies.append(history.history['accuracy'])\n",
    "    train_losses.append(history.history['loss'])\n",
    "    val_accuracies.append(history.history['val_accuracy'])\n",
    "    val_losses.append(history.history['val_loss'])\n",
    "    epochs_per_fold.append(len(history.history['accuracy']))\n",
    "\n",
    "# Calculate average accuracy and loss\n",
    "avg_train_accuracy = np.sum([np.sum(acc) for acc in train_accuracies]) / np.sum(epochs_per_fold)\n",
    "avg_train_loss = np.sum([np.sum(loss) for loss in train_losses]) / np.sum(epochs_per_fold)\n",
    "avg_val_accuracy = np.sum([np.sum(acc) for acc in val_accuracies]) / np.sum(epochs_per_fold)\n",
    "avg_val_loss = np.sum([np.sum(loss) for loss in val_losses]) / np.sum(epochs_per_fold)\n",
    "\n",
    "# Print average accuracy and loss\n",
    "print(f\"Average train accuracy: {avg_train_accuracy}\")\n",
    "print(f\"Average train loss: {avg_train_loss}\")\n",
    "print(f\"Average validation accuracy: {avg_val_accuracy}\")\n",
    "print(f\"Average validation loss: {avg_val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Train and validate the model\n",
    "# accuracies = []\n",
    "# losses = []\n",
    "#\n",
    "# for train_index, val_index in tqdm(skf.split(X, np.argmax(y, axis=1)), total=k, desc=\"Training progress\"):\n",
    "#     X_train, X_val = X[train_index], X[val_index]\n",
    "#     y_train, y_val = y[train_index], y[val_index]\n",
    "#\n",
    "#     model = create_lstm_model((X_train.shape[1], X_train.shape[2]))\n",
    "#\n",
    "#     history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
    "#\n",
    "#     accuracies.append(history.history['val_accuracy'])\n",
    "#     losses.append(history.history['val_loss'])\n",
    "#\n",
    "# # Calculate average accuracy and loss\n",
    "# avg_accuracy = np.mean(accuracies, axis=0)\n",
    "# avg_loss = np.mean(losses, axis=0)\n",
    "#\n",
    "# print(avg_accuracy)\n",
    "# print(avg_loss)\n",
    "#\n",
    "# # Plot ROC curve for accuracy and loss\n",
    "# def plot_roc_curve(avg_accuracy, avg_loss):\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#\n",
    "#     # Plot accuracy\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.plot(avg_accuracy, label='Validation')\n",
    "#     plt.title('Model Accuracy')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.legend(loc='lower right')\n",
    "#\n",
    "#     # Plot loss\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.plot(history.history['loss'], label='Train')\n",
    "#     plt.plot(history.history['val_loss'], label='Validation')\n",
    "#     plt.title('Model Loss')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.legend(loc='upper right')\n",
    "#\n",
    "#     plt.show()\n",
    "#\n",
    "# plot_roc_curve(avg_accuracy, avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # Plot ROC curve for accuracy and loss\n",
    "# def plot_roc_curve(history):\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#\n",
    "#     # Plot accuracy\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.plot(history.history['accuracy'], label='Train')\n",
    "#     plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "#     plt.title('Model Accuracy')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.legend(loc='lower right')\n",
    "#\n",
    "#     # Plot loss\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.plot(history.history['loss'], label='Train')\n",
    "#     plt.plot(history.history['val_loss'], label='Validation')\n",
    "#     plt.title('Model Loss')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.legend(loc='upper right')\n",
    "#\n",
    "#     plt.show()\n",
    "#\n",
    "# plot_roc_curve(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(avg_train_accuracy, avg_train_loss, avg_val_accuracy, avg_val_loss):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(avg_train_accuracy, label='Train')\n",
    "    plt.plot(avg_val_accuracy, label='Validation')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(avg_train_loss, label='Train')\n",
    "    plt.plot(avg_val_loss, label='Validation')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Plot the results\n",
    "plot_roc_curve(avg_train_accuracy[:max_epochs_reached], avg_train_loss[:max_epochs_reached], avg_val_accuracy[:max_epochs_reached], avg_val_loss[:max_epochs_reached])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Split the dataset into features and labels\n",
    "X = sensor_data.drop([\"activityID\", \"timestamp\", \"subjectID\"], axis=1)\n",
    "y = sensor_data[\"activityID\"]\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=128, input_shape=(X.shape[1], 1), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=64, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=32))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=len(np.unique(y)), activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Define the number of folds for stratified k-fold cross-validation\n",
    "k = 5\n",
    "\n",
    "# Split the dataset into training and testing sets using stratified k-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Reshape the data for input to the LSTM model\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    # Train the LSTM model\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "y_pred = model.predict_classes(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", confusion_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Observe the removal of miss-classified activities and transient activities with ID zero: 929662 items\n",
    "(1942871, 20)\n",
    "There is only one misclassified instance for subject 104, where they were predicted to have performed the running activity. However, according to the activities performed document, subject 104 never performed this activity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create a dictionary that maps the original label to a new sequential label and then use the replace method of the sensor_data DataFrame to replace the original labels with the new sequential labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class_counts = sensor_data['activityID'].value_counts().sort_index()\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # Get the unique activity IDs from the data\n",
    "unique_activities = sorted(sensor_data['activityID'].unique())\n",
    "\n",
    "# Create a label mapping from activity IDs to integers\n",
    "label_map = {activity_id: i for i, activity_id in enumerate(unique_activities)}\n",
    "\n",
    "# Replace activity IDs with their integer labels\n",
    "sensor_data['activityID'] = sensor_data['activityID'].apply(lambda x: label_map[x])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To print out the population of each class in the activityID column of the sensor_data dataframe, you can use the value_counts() method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class_counts = sensor_data['activityID'].value_counts().sort_index()\n",
    "print(class_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Count the number of samples per subject-activity combination. You can use the groupby function in Pandas to group the data by both subjectID and activityID, and then use the size() function to count the number of samples in each group.\n",
    "This will print the number of samples per subject-activity combination. You can check if any combination has less than two samples, which is causing the error. If you find such a combination, you can either remove it from the dataset or merge it with another similar combination to make sure there are at least two samples in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "group_counts = sensor_data.groupby(['subjectID', 'activityID']).size()\n",
    "print(group_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # Preprocess the data\n",
    "# X = sensor_data.drop([\"timestamp\", \"activityID\", \"subjectID\"], axis=1) # Ignore first 3 columns (timestamp, subjectID, activityID)\n",
    "# y = sensor_data[[\"subjectID\", \"activityID\"]].values\n",
    "#\n",
    "# # Split the data into training and testing sets\n",
    "# from sklearn.model_selection import train_test_split\n",
    "#\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "#\n",
    "# # Define the window size and stride\n",
    "# window_size = 100\n",
    "# stride = 50\n",
    "#\n",
    "# # Group the data by subject and activity ID\n",
    "# grouped_data = sensor_data.groupby(['subjectID', 'activityID'], sort=False)\n",
    "#\n",
    "# # Apply a window function to each group\n",
    "# def apply_window_function(group):\n",
    "#     windows = []\n",
    "#     for i in range(0, len(group)-window_size+1, stride):\n",
    "#         window = group.iloc[i:i+window_size]\n",
    "#         window_label = window['activityID'].mode()[0]  # Assign the majority class label to the window\n",
    "#         window_subject = window['subjectID'].unique()[0]  # Assign the subject ID to the window\n",
    "#         window = window.drop(['subjectID', 'activityID'], axis=1)  # Drop the subject ID and activity ID columns from the window data\n",
    "#         window = np.asarray(window)  # Convert the window data to a numpy array\n",
    "#         windows.append((window, window_label, window_subject))\n",
    "#     return windows\n",
    "#\n",
    "# grouped_windows = grouped_data.apply(apply_window_function)\n",
    "# windows = [window for group in grouped_windows for window in group]\n",
    "#\n",
    "# # Split the windows into train and test sets, stratifying across both subject and activity IDs\n",
    "# X = [window[0] for window in windows]\n",
    "# y = [window[1] for window in windows]\n",
    "# subject_ids = [window[2] for window in windows]\n",
    "# X_train, X_test, y_train, y_test, subject_ids_train, subject_ids_test = train_test_split(X, y, subject_ids, test_size=0.2, random_state=42, shuffle=True, stratify=[y, subject_ids])\n",
    "#\n",
    "# # Convert the train and test sets to pandas dataframes\n",
    "# train_data = pd.DataFrame({'data': X_train, 'activityID': y_train, 'subjectID': subject_ids_train})\n",
    "# test_data = pd.DataFrame({'data': X_test, 'activityID': y_test, 'subjectID': subject_ids_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "#\n",
    "# # Define window size and stride\n",
    "# window_size = 100\n",
    "# stride = 50\n",
    "#\n",
    "# # Create a list of all possible windows\n",
    "# windows = []\n",
    "# for subject, activity in sensor_data.groupby(['subjectID', 'activityID']):\n",
    "#     subject_id = subject[0]\n",
    "#     activity_id = activity['activityID'].iloc[0]\n",
    "#     data = activity.drop(['subjectID', 'activityID'], axis=1)\n",
    "#     for i in range(0, len(data)-window_size+1, stride):\n",
    "#         window = data.iloc[i:i+window_size, :]\n",
    "#         #windows.append([window.values, subject_id, activity_id])\n",
    "#\n",
    "# # Convert the list of windows to a DataFrame\n",
    "# window_data = pd.DataFrame(windows, columns=['data', 'subjectID', 'activityID'])\n",
    "# print(window_data.head(1))\n",
    "\n",
    "# # Shuffle the windows\n",
    "# window_data = window_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "#\n",
    "# # Extract the data and labels from the DataFrame\n",
    "# X = np.stack(window_data['data'].values)\n",
    "# y_activity = window_data['activityID'].values\n",
    "# y_subject = window_data['subjectID'].values\n",
    "#\n",
    "# # Split the data into train and test sets, stratifying across both subject and activity IDs\n",
    "# grouped_data = window_data.groupby(['subjectID', 'activityID'], sort=False)\n",
    "# grouped_data_str = grouped_data[['subjectID', 'activityID']].apply(lambda x: '_'.join(x.astype(str)), axis=1)\n",
    "# train_subjects, test_subjects = train_test_split(grouped_data_str, test_size=0.2, random_state=42, shuffle=True, stratify=grouped_data_str)\n",
    "# train_subjects = pd.DataFrame({'subjectID': train_subjects.str.split('_').str[0].values, 'activityID': train_subjects.str.split('_').str[1].values})\n",
    "# test_subjects = pd.DataFrame({'subjectID': test_subjects.str.split('_').str[0].values, 'activityID': test_subjects.str.split('_').str[1].values})\n",
    "#\n",
    "# train_data = window_data.merge(train_subjects, on=['subjectID', 'activityID'])\n",
    "# test_data = window_data.merge(test_subjects, on=['subjectID', 'activityID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Define window size and stride\n",
    "window_size = 100\n",
    "stride = 50\n",
    "\n",
    "features = ['mean', 'std', 'min', 'max', '25th', '50th', '75th']\n",
    "\n",
    "feature_columns = []\n",
    "for sensor in fields[2:]:\n",
    "    for feature in features:\n",
    "        feature_column = sensor + '_' + feature\n",
    "        feature_columns.append(feature_column)\n",
    "\n",
    "print(len(feature_columns))\n",
    "feature_set = pd.DataFrame(columns=feature_columns)\n",
    "\n",
    "feature_set.head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from statistics import mode\n",
    "#\n",
    "# # Create a list of all possible windows\n",
    "# windows = []\n",
    "# for subject, activity in tqdm(sensor_data.groupby(['subjectID', 'activityID'])):\n",
    "#     subject_id = subject[0]\n",
    "#     activity_id = mode(activity['activityID']) # Implement majority voting\n",
    "#     data = activity.drop(['subjectID', 'timestamp', 'activityID'], axis=1)\n",
    "#\n",
    "#     for i in range(0, len(data)-window_size+1, stride):\n",
    "#         window = data.iloc[i:i+window_size, :]\n",
    "#\n",
    "#         # Extract features from the window\n",
    "#         window_features = []\n",
    "#         for col in range(window.shape[1]):\n",
    "#             col_data = window.iloc[:, col].values\n",
    "#             window_features.append(np.mean(col_data)) # mean\n",
    "#             window_features.append(np.std(col_data)) # standard deviation\n",
    "#             window_features.append(np.min(col_data)) # minimum\n",
    "#             window_features.append(np.max(col_data)) # maximum\n",
    "#             window_features.append(np.percentile(col_data, 25)) # 25th percentile\n",
    "#             window_features.append(np.percentile(col_data, 50)) # 50th percentile\n",
    "#             window_features.append(np.percentile(col_data, 75)) # 75th percentile\n",
    "#\n",
    "#         # Add the feature vector and label to the feature matrix and labels array\n",
    "#         window_data = [subject_id, activity_id] + window_features\n",
    "#         windows.append(window_data)\n",
    "# print(windows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "#\n",
    "# # normalise the dataset\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# normalised_data = scaler.fit_transform(sensor_data)\n",
    "# normalised_data.tail(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Select the subset of columns for input data\n",
    "data = sensor_data.iloc[:, 3:]\n",
    "\n",
    "# Define window size and stride length\n",
    "window_size = 100\n",
    "stride_length = 50\n",
    "\n",
    "# Create empty numpy arrays for input and output data\n",
    "X = np.empty((0, window_size, data.shape[1]))\n",
    "y = np.empty((0,))\n",
    "\n",
    "# Loop through the data and create windows\n",
    "for i in tqdm(range(0, len(data) - window_size, stride_length)):\n",
    "    # Get the window and the corresponding label\n",
    "    window = data.iloc[i:i+window_size, :]\n",
    "    label = sensor_data.iloc[i+window_size]['activityID']\n",
    "\n",
    "    # Append the window and label to the input and output data arrays\n",
    "    X = np.append(X, [window.values], axis=0)\n",
    "    y = np.append(y, [label], axis=0)\n",
    "\n",
    "# Reshape the input data to have dimensions (num_windows, window_size, num_features)\n",
    "X = np.reshape(X, (X.shape[0], window_size, data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define number of output classes and features from input shape\n",
    "num_features = X.shape[2]\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=64, input_shape=(window_size, num_features)))\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Time Series Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Define the number of splits for cross-validation\n",
    "n_splits = 5\n",
    "\n",
    "# Initialize time series cross-validation object\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Loop over the splits\n",
    "for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "\n",
    "    # Get the training and test data for this split\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Train the model on the training data\n",
    "    # model.fit(X_train, y_train, verbose=1)\n",
    "    histpry = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy for split {i+1}: {accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this code block, we use the TimeSeriesSplit function from scikit-learn to perform time series cross-validation. We define the number of folds using the n_splits parameter, and then use the split method to generate indices for the training and test sets for each fold. We then use these indices to extract the corresponding subsets of the input and target data, and train and evaluate the model on each fold separately."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Sliding Window Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the window size and step size\n",
    "window_size = 100\n",
    "step_size = 50\n",
    "\n",
    "# Loop over the windows\n",
    "for i in range(0, len(X) - window_size, step_size):\n",
    "\n",
    "    # Get the training and test data for this window\n",
    "    X_train = X[i:i+window_size]\n",
    "    y_train = y[i:i+window_size]\n",
    "    X_test = X[i+window_size:i+window_size+step_size]\n",
    "    y_test = y[i+window_size:i+window_size+step_size]\n",
    "\n",
    "    # Train the model on the training data\n",
    "    # model.fit(X_train, y_train, verbose=1)\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy for window starting at index {i}: {accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this code block, we perform sliding window cross-validation by generating a series of overlapping windows over the training and test data. We define the window size and step size using the window_size and step_size parameters, and then use a generate_windows function (not shown) to generate overlapping windows of input and target data. We then train the model on each window of the training data, and evaluate it on the corresponding window of the test data. The j variable is used to calculate the index of the corresponding test window for each training window."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define window size and stride\n",
    "window_size = 100\n",
    "stride = 50\n",
    "\n",
    "# Create empty lists for windows and labels\n",
    "X_windows = []\n",
    "y_labels = []\n",
    "\n",
    "# Loop over all windows and extract features\n",
    "for subject, activity in tqdm(sensor_data.groupby(['subjectID', 'activityID'])):\n",
    "    data = activity.drop(['subjectID', 'activityID'], axis=1)\n",
    "    label = activity['activityID'].iloc[0]\n",
    "\n",
    "    for i in range(0, len(data)-window_size+1, stride):\n",
    "        window = data.iloc[i:i+window_size, :]\n",
    "        X_windows.append(window.values.flatten())\n",
    "        y_labels.append(label)\n",
    "\n",
    "# Convert windows and labels to numpy arrays\n",
    "X_windows = np.array(X_windows)\n",
    "y_labels = np.array(y_labels)\n",
    "\n",
    "# Normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_windows[:, 3:] = scaler.fit_transform(X_windows[:, 3:])\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_labels = to_categorical(y_labels)\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_windows, y_labels, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape X_train, X_test, and X_val to be 3D arrays with shape (n_samples, window_size, n_features)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0]//window_size, window_size, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0]//window_size, window_size, X_test.shape[1]))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0]//window_size, window_size, X_val.shape[1]))\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an LSTM layer with 64 units and input shape of (window_size, num_features)\n",
    "model.add(LSTM(units=64, input_shape=(window_size, X_train.shape[2])))\n",
    "\n",
    "# Add a dropout layer to prevent overfitting\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add a second LSTM layer with 32 units\n",
    "model.add(LSTM(units=32))\n",
    "\n",
    "# Add a dropout layer to prevent overfitting\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add a dense layer with 12 units (one for each activity class) and softmax activation function\n",
    "model.add(Dense(units=12, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss}, test accuracy: {accuracy}')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(sensor_data)\n",
    "\n",
    "# Define window size and stride\n",
    "window_size = 100\n",
    "stride = 50\n",
    "\n",
    "# Create empty lists for windows and labels\n",
    "X_windows = []\n",
    "y_labels = []\n",
    "\n",
    "# Loop over all windows and extract features\n",
    "for subject, activity in tqdm(sensor_data.groupby(['subjectID', 'activityID'])):\n",
    "    data = activity.drop(['subjectID', 'activityID'], axis=1)\n",
    "    label = activity['activityID'].iloc[0]\n",
    "\n",
    "    for i in range(0, len(data)-window_size+1, stride):\n",
    "        window = data.iloc[i:i+window_size, :]\n",
    "        X_windows.append(window.values)\n",
    "        y_labels.append(label)\n",
    "\n",
    "# Convert windows and labels to numpy arrays\n",
    "X_windows = np.array(X_windows)\n",
    "y_labels = np.array(y_labels)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_labels = to_categorical(y_labels)\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_windows, y_labels, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an LSTM layer with 64 units and input shape of (window_size, num_features)\n",
    "model.add(LSTM(units=64, input_shape=(window_size, X_train.shape[2])))\n",
    "\n",
    "# Add a dropout layer to prevent overfitting\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add a second LSTM layer with 32 units\n",
    "model.add(LSTM(units=32))\n",
    "\n",
    "# Add a dropout layer to prevent overfitting\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add a dense layer with 12 units (one for each activity class) and softmax activation function\n",
    "model.add(Dense(units=12, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss}, test accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from statistics import mode\n",
    "\n",
    "# Define window size and stride\n",
    "window_size = 100\n",
    "stride = 50\n",
    "\n",
    "# Create an empty list for the windowed data\n",
    "windowed_data_arr = []\n",
    "\n",
    "# Loop over all windows and extract the data\n",
    "for subject, activity in tqdm(sensor_data.groupby(['subjectID', 'activityID'])):\n",
    "    subject_id = int(subject[0])\n",
    "    activity_id = int(mode(activity['activityID'])) # Implement majority voting\n",
    "    data = activity.drop(['subjectID', 'activityID'], axis=1)\n",
    "\n",
    "    for i in range(0, len(data)-window_size+1, stride):\n",
    "        window = data.iloc[i:i+window_size, :]\n",
    "        windowed_data_arr.append((subject_id, activity_id, window.values))\n",
    "\n",
    "# Convert the windowed data to a dataframe\n",
    "windowed_data_df = pd.DataFrame(windowed_data_arr, columns=['subjectID', 'activityID', 'window'])\n",
    "windowed_data_df.tail(20)\n",
    "windowed_data_df.shape\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Feature selection matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a correlation matrix of the feature_set dataframe\n",
    "corr_matrix = feature_set.drop(['activityID', 'subjectID'], axis=1).corr()\n",
    "\n",
    "# Plot the correlation matrix using a heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, cmap='Blues')\n",
    "plt.title('Feature Selection Matrix')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "# Split the feature set and target variable\n",
    "X = feature_set.drop(columns=['activityID'])\n",
    "y = feature_set['activityID']\n",
    "\n",
    "# Select the top 20 features based on mutual information\n",
    "selector = SelectKBest(mutual_info_classif, k=20)\n",
    "selector.fit_transform(X, y)\n",
    "\n",
    "# Get the indices of the selected features\n",
    "selected_indices = selector.get_support(indices=True)\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_features = X.columns[selected_indices]\n",
    "\n",
    "# Create a new feature set with the selected features\n",
    "selected_feature_set = feature_set[['activityID'] + list(selected_features)]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a correlation matrix of the feature_set dataframe\n",
    "corr_matrix = selected_feature_set.drop(columns=['activityID']).corr()\n",
    "\n",
    "# Plot the correlation matrix using a heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, cmap='Blues')\n",
    "plt.title('Feature Selection Matrix')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Testing on the full 126 extracted features from each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # Drop subjectID and activityID columns\n",
    "# X = feature_set.drop(['subjectID', 'activityID'], axis=1)\n",
    "#\n",
    "# # Get target variable (activityID)\n",
    "# y = feature_set['activityID']\n",
    "#\n",
    "# # Split into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "#\n",
    "# # Define the model\n",
    "# model = Sequential()\n",
    "#\n",
    "# # Add an LSTM layer with 64 units and input shape of (window_size, num_features)\n",
    "# model.add(LSTM(units=64, input_shape=(window_size, feature_set.shape[1]-2)))\n",
    "#\n",
    "# # Add a dropout layer to prevent overfitting\n",
    "# model.add(Dropout(0.5))\n",
    "#\n",
    "# # Add a dense layer with 12 units (one for each activity class)\n",
    "# model.add(Dense(units=12, activation='softmax'))\n",
    "#\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "#\n",
    "# # Drop subjectID and activityID columns\n",
    "# X = feature_set.drop(['subjectID', 'activityID'], axis=1)\n",
    "#\n",
    "# # Get target variable (activityID)\n",
    "# y = feature_set['activityID']\n",
    "#\n",
    "# # Split into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#\n",
    "# # Define the model\n",
    "# model = Sequential()\n",
    "#\n",
    "# # Add an LSTM layer with 64 units and input shape of (window_size, num_features)\n",
    "# model.add(LSTM(units=64, input_shape=(window_size, feature_set.shape[1]-2)))\n",
    "#\n",
    "# # Add a dropout layer to prevent overfitting\n",
    "# model.add(Dropout(0.5))\n",
    "#\n",
    "# # Add a dense layer with 12 units (one for each activity class)\n",
    "# model.add(Dense(units=12, activation='softmax'))\n",
    "#\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#\n",
    "# # Fit the model\n",
    "# model.fit(X_train, y_train, epochs=50, batch_size=64, verbose=1)\n",
    "#\n",
    "# # Make predictions on test data\n",
    "# y_pred = model.predict(X_test)\n",
    "#\n",
    "# # Convert probabilities to class labels\n",
    "# y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "https://stackoverflow.com/questions/74917051/tensorflow-error-on-macbook-m1-pro-notfounderror-graph-execution-error\n",
    "conda install -c apple tensorflow-deps=2.9.0\n",
    "python -m pip install tensorflow-macos==2.9\n",
    "pip install tensorflow-deps==2.9.0\n",
    "pip install tensorflow-metal==0.5.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Drop subjectID and activityID columns\n",
    "X = windowed_data_df.drop(['subjectID', 'activityID'], axis=1)\n",
    "\n",
    "# Get target variable (activityID)\n",
    "y = windowed_data_df['activityID']\n",
    "\n",
    "# Split into training, testing, and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Reshape data into 3D format\n",
    "X_train = X_train.to_numpy().reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.to_numpy().reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "X_val = X_val.to_numpy().reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an LSTM layer with 64 units and input shape of (num_features,)\n",
    "model.add(LSTM(units=64, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "\n",
    "# Add a dropout layer to prevent overfitting\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add a second LSTM layer with 32 units\n",
    "model.add(LSTM(units=32))\n",
    "\n",
    "# Add a dropout layer to prevent overfitting\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add a dense layer with 12 units (one for each activity class) and softmax activation function\n",
    "model.add(Dense(units=12, activation='softmax'))\n",
    "\n",
    "# One-hot encode the target variable\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_val = to_categorical(y_val)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss}, test accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Drop subjectID and activityID columns\n",
    "X = windowed_data_df.drop(['subjectID', 'activityID'], axis=1)\n",
    "\n",
    "# Get target variable (activityID)\n",
    "y = windowed_data_df['activityID']\n",
    "\n",
    "# One-hot encode the target variable\n",
    "y = to_categorical(y)\n",
    "\n",
    "# Split into training, testing, and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Reshape training data into 3D format\n",
    "X_train = np.reshape(X_train.to_numpy(), (X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "# Reshape validation data into 3D format\n",
    "X_val = np.reshape(X_val.to_numpy(), (X_val.shape[0], 1, X_val.shape[1]))\n",
    "\n",
    "# Reshape testing data into 3D format\n",
    "X_test = np.reshape(X_test.to_numpy(), (X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an LSTM layer with 64 units and input shape of (num_features,)\n",
    "model.add(LSTM(units=64, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "\n",
    "# Add a dropout layer to prevent overfitting\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add a second LSTM layer with 32 units\n",
    "model.add(LSTM(units=32))\n",
    "\n",
    "# Add a dropout layer to prevent overfitting\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add a dense layer with 12 units (one for each activity class) and softmax activation function\n",
    "model.add(Dense(units=12, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(x_val, y_val), verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss}, test accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "#\n",
    "# # Drop subjectID and activityID columns\n",
    "# X = feature_set.drop(['subjectID', 'activityID'], axis=1)\n",
    "#\n",
    "# # Get target variable (activityID)\n",
    "# y = feature_set['activityID']\n",
    "#\n",
    "# # Split into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#\n",
    "# # Define the model\n",
    "# model = Sequential()\n",
    "#\n",
    "# # Add an LSTM layer with 64 units and input shape of (num_features,)\n",
    "# model.add(LSTM(units=64, input_shape=(X_train.shape[1],)))\n",
    "#\n",
    "# # Add a dropout layer to prevent overfitting\n",
    "# model.add(Dropout(0.5))\n",
    "#\n",
    "# # Add a dense layer with 12 units (one for each activity class)\n",
    "# model.add(Dense(units=12, activation='softmax'))\n",
    "#\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#\n",
    "# # Fit the model\n",
    "# model.fit(X_train, y_train, epochs=50, batch_size=64, verbose=1)\n",
    "#\n",
    "# # Make predictions on test data\n",
    "# y_pred = model.predict(X_test)\n",
    "#\n",
    "# # Convert probabilities to class labels\n",
    "# y_pred = np.argmax(y_pred, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "target_names = ['class {}'.format(i) for i in range(12)]\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "#\n",
    "# # Define window size and stride\n",
    "# window_size = 100\n",
    "# stride = 50\n",
    "#\n",
    "# # Create a list of all possible windows\n",
    "# feature_set = pd.DataFrame()\n",
    "# for subject, activity in tqdm(sensor_data.groupby(['subjectID', 'activityID'])):\n",
    "#     subject_id = subject[0]\n",
    "#     activity_id = activity['activityID'].iloc[0] # Implement majority voting\n",
    "#     data = activity.drop(['subjectID', 'timestamp', 'activityID'], axis=1)\n",
    "#     for i in range(0, len(data)-window_size+1, stride):\n",
    "#         window = data.iloc[i:i+window_size, :]\n",
    "#\n",
    "#         # Extract features from the window\n",
    "#         window_features = []\n",
    "#         for col in range(window.shape[1]):\n",
    "#             col_data = window.iloc[:, col].values\n",
    "#             window_features.append(np.mean(col_data)) # mean\n",
    "#             window_features.append(np.std(col_data)) # standard deviation\n",
    "#             window_features.append(np.min(col_data)) # minimum\n",
    "#             window_features.append(np.max(col_data)) # maximum\n",
    "#             window_features.append(np.percentile(col_data, 25)) # 25th percentile\n",
    "#             window_features.append(np.percentile(col_data, 50)) # 50th percentile\n",
    "#             window_features.append(np.percentile(col_data, 75)) # 75th percentile\n",
    "#\n",
    "#         # Add the feature vector and label to the feature matrix and labels array\n",
    "#         window_data = [subject_id, activity_id] + window_features\n",
    "#         windows.append(window_data)\n",
    "# print(windows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# print(len(windows[0]))\n",
    "# print(len(windows))\n",
    "# print(len(sensor_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter data for subjectID = 1 and protocol activities only\n",
    "subject_data = sensor_data[(sensor_data['subjectID'] == 1) & (sensor_data['activityID'].isin(protocol_acts))]\n",
    "\n",
    "# Group data by activityID\n",
    "grouped_data = subject_data.groupby('activityID')\n",
    "\n",
    "# Set up plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Define colors and labels for each activityID\n",
    "colours = {0: 'red', 1: 'blue', 2: 'green', 3: 'orange', 4: 'purple', 5: 'brown', 6: 'pink', 7: 'gray', 8: 'olive', 9: 'cyan', 10: 'magenta', 11: 'yellow'}\n",
    "labels = {0: 'lying', 1: 'sitting', 2: 'standing', 3: 'walking', 4: 'running', 5: 'cycling', 6: 'Nordic walking', 7: 'ascending stairs', 8: 'descending stairs', 9: 'vacuum cleaning', 10: 'ironing', 11: 'rope jumping'}\n",
    "\n",
    "# Loop through each activityID and plot corresponding data\n",
    "activity_id = []\n",
    "x_means = []\n",
    "y_means = []\n",
    "z_means = []\n",
    "for window_data in windows:\n",
    "    if window_data[0] == 1: # if subject id is 1\n",
    "        activity_id.append(window_data[1])\n",
    "        x_means.append(window_data[79]) # mean for chestGyro1 x dimension\n",
    "        y_means.append(window_data[86]) # mean for chestGyro2 y dimension\n",
    "        z_means.append(window_data[93]) # mean for chestGyro3 z dimension\n",
    "print(activity_id)\n",
    "print(x_means)\n",
    "print(y_means)\n",
    "print(z_means)\n",
    "\n",
    "# Create scatter plot with colored points\n",
    "for i in range(len(activity_id)):\n",
    "    ax.scatter(x_means[i], y_means[i], color=colours[activity_id[i]])\n",
    "\n",
    "# Add axis labels and title\n",
    "ax.set_xlabel('Mean Chest Gyro1 X Dimension')\n",
    "ax.set_ylabel('Mean Chest Gyro2 Y Dimension')\n",
    "ax.set_title('Activity Clustering for Subject 1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter data for subjectID = 1 and protocol activities only\n",
    "subject_data = sensor_data[(sensor_data['subjectID'] == 1) & (sensor_data['activityID'].isin(protocol_acts))]\n",
    "\n",
    "# Group data by activityID\n",
    "grouped_data = subject_data.groupby('activityID')\n",
    "\n",
    "# Define colors and labels for each activityID\n",
    "colours = {0: 'red', 1: 'blue', 2: 'green', 3: 'orange', 4: 'purple', 5: 'brown', 6: 'pink', 7: 'gray', 8: 'olive', 9: 'cyan', 10: 'magenta', 11: 'yellow'}\n",
    "labels = {0: 'lying', 1: 'sitting', 2: 'standing', 3: 'walking', 4: 'running', 5: 'cycling', 6: 'Nordic walking', 7: 'ascending stairs', 8: 'descending stairs', 9: 'vacuum cleaning', 10: 'ironing', 11: 'rope jumping'}\n",
    "\n",
    "# Initialize 3D plot\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Initialize subplot\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Set subplot size and position\n",
    "fig.subplots_adjust(left=0.05, right=1, bottom=0.05, top=0.95)\n",
    "\n",
    "# Loop through each activityID and plot corresponding data\n",
    "activity_id, x_means, y_means, z_means = [], [], [], []\n",
    "for window_data in windows:\n",
    "    if window_data[0] == 1: # if subject id is 1\n",
    "        activity_id.append(window_data[1])\n",
    "        x_means.append(window_data[79]) # mean for chestGyro1 x dimension\n",
    "        y_means.append(window_data[86]) # mean for chestGyro2 y dimension\n",
    "        z_means.append(window_data[93]) # mean for chestGyro3 z dimension\n",
    "\n",
    "# Create scatter plot with colored points and legend labels\n",
    "for i in range(len(activity_id)):\n",
    "    ax.scatter(x_means[i], y_means[i], z_means[i], color=colours[activity_id[i]])\n",
    "\n",
    "# Add axis labels and title\n",
    "ax.set_xlabel('Mean Chest Gyro1 X Dimension')\n",
    "ax.set_ylabel('Mean Chest Gyro2 Y Dimension')\n",
    "ax.set_zlabel('Mean Chest Gyro3 Z Dimension', labelpad=15)\n",
    "ax.set_title('Activity Clustering for Subject 1')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Visualise the stratified test train split by plotting the distribution of both the target variable activityID and subjectID in each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Preprocess the data\n",
    "# X = sensor_data.drop([\"timestamp\", \"activityID\"], axis=1) # Ignore columns timestamp and activityID\n",
    "X = sensor_data\n",
    "y = sensor_data[\"activityID\"].values\n",
    "stratify_by = sensor_data[[\"subjectID\", \"activityID\"]].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=stratify_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of activityID in the train and test sets\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10,5))\n",
    "sns.countplot(x=y_train, ax=ax[0])\n",
    "ax[0].set_title(\"Distribution of activityID in Train set\")\n",
    "sns.countplot(x=y_test, ax=ax[1])\n",
    "ax[1].set_title(\"Distribution of activityID in Test set\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the distribution of subjectID in the train and test sets\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10,5))\n",
    "sns.countplot(x=X_train[\"subjectID\"], ax=ax[0])\n",
    "ax[0].set_title(\"Distribution of subjectID in Train set\")\n",
    "sns.countplot(x=X_test[\"subjectID\"], ax=ax[1])\n",
    "ax[1].set_title(\"Distribution of subjectID in Test set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Drop the subjectID column from X_train and X_test after we have stratified\n",
    "X_train = X_train.drop(\"subjectID\", axis=1)\n",
    "X_test = X_test.drop(\"subjectID\", axis=1)\n",
    "# Also remove the column from the original feature set for later use\n",
    "X = sensor_data.drop([\"subjectID\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Count the number of instances for each activity in the training set\n",
    "# train_activity_counts = pd.Series(y_train).value_counts()\n",
    "#\n",
    "# # Count the number of instances for each activity in the testing set\n",
    "# test_activity_counts = pd.Series(y_test).value_counts()\n",
    "#\n",
    "# # Plot the activity counts for the training set and testing set\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.bar(train_activity_counts.index, train_activity_counts.values)\n",
    "# plt.title(\"Training Set Activity Counts\")\n",
    "# plt.xlabel(\"Activity ID\")\n",
    "# plt.ylabel(\"Number of Instances\")\n",
    "#\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.bar(test_activity_counts.index, test_activity_counts.values)\n",
    "# plt.title(\"Testing Set Activity Counts\")\n",
    "# plt.xlabel(\"Activity ID\")\n",
    "# plt.ylabel(\"Number of Instances\")\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "#\n",
    "# # Preprocess the data\n",
    "# X = sensor_data.drop([\"timestamp\", \"activityID\", \"subjectID\"], axis=1) # Ignore first 3 columns (timestamp, subjectID, activityID)\n",
    "# y_activity = sensor_data[\"activityID\"].values\n",
    "# y_subject = sensor_data[\"subjectID\"].values\n",
    "#\n",
    "# # Split the data into training and testing sets\n",
    "# from sklearn.model_selection import train_test_split\n",
    "#\n",
    "# X_train, X_test, y_activity_train, y_activity_test, y_subject_train, y_subject_test = train_test_split(X, y_activity, y_subject, test_size=0.2, random_state=42, stratify=y_activity)\n",
    "#\n",
    "# # Plot the count of activity IDs in the dataset\n",
    "# plt.figure(figsize=(8,6))\n",
    "# sns.countplot(x=y_activity)\n",
    "# plt.title(\"Distribution of Activity IDs\")\n",
    "# plt.xlabel(\"Activity ID\")\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.show()\n",
    "#\n",
    "# # Plot the count of subject IDs in the dataset\n",
    "# plt.figure(figsize=(8,6))\n",
    "# sns.countplot(x=y_subject)\n",
    "# plt.title(\"Distribution of Subject IDs\")\n",
    "# plt.xlabel(\"Subject ID\")\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Reshape the data for LSTM input\n",
    "# n_timesteps, n_features = X.shape[0], X.shape[1]\n",
    "# X_train = X_train.reshape((X_train.shape[0], n_timesteps, n_features))\n",
    "# X_test = X_test.reshape((X_test.shape[0], n_timesteps, n_features))\n",
    "#\n",
    "# # Build the model\n",
    "# model = tf.keras.Sequential()\n",
    "# model.add(layers.LSTM(100, input_shape=(n_timesteps, n_features)))\n",
    "# model.add(layers.Dense(100, activation='relu'))\n",
    "# model.add(layers.Dense(y.shape[1], activation='softmax'))\n",
    "#\n",
    "# # Compile the model\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#\n",
    "# # Train the model\n",
    "# history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
    "#\n",
    "# # Evaluate the model\n",
    "# loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "# print(f\"Test loss: {loss:.3f}\")\n",
    "# print(f\"Test accuracy: {accuracy:.3f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this code, we are dropping the `subject_id` and `activity_id` columns from the feature set `X`, and using the activity_id column as the label `y`. We then use the `train_test_split` function to split the data into train and test sets, with a test size of 0.2 and a random state of 42. We also use the `stratify` parameter to ensure that the train and test sets are stratified by both `subject_id` and `activity_id` whilst still preserving the order of timesteps within each window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "#\n",
    "# # Group the data by subject and activity ID\n",
    "# grouped_data = sensor_data.groupby(['subjectID', 'activityID'], sort=False)\n",
    "#\n",
    "# # Split the data into train and test sets, stratifying across both subject and activity IDs\n",
    "# train_subjects, test_subjects = train_test_split(grouped_data, test_size=0.2, random_state=42, shuffle=True, stratify=grouped_data[['subjectID', 'activityID']])\n",
    "#\n",
    "# # Concatenate the train and test sets back into single dataframes\n",
    "# train_data = pd.concat([data for _, data in train_subjects], ignore_index=True)\n",
    "# test_data = pd.concat([data for _, data in test_subjects], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "#\n",
    "# # Split the data into training and testing sets with stratification\n",
    "# train_data, test_data = train_test_split(sensor_data, stratify=(sensor_data['subjectID'], sensor_data['activityID']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "#\n",
    "# # Group the data by subject and activity ID\n",
    "# grouped_data = sensor_data.groupby(['subjectID', 'activityID'], sort=False)\n",
    "#\n",
    "# # Split the data into train and test sets, stratifying across both subject and activity IDs\n",
    "# train_subjects, test_subjects = train_test_split(grouped_data, test_size=0.2, random_state=42, shuffle=True, stratify=grouped_data[['subjectID', 'activityID']].apply(lambda x: x.str.cat(sep='_')))\n",
    "#\n",
    "# # Concatenate the train and test sets back into single dataframes\n",
    "# train_data = pd.concat([data for _, data in train_subjects], ignore_index=True, sort=False)\n",
    "# test_data = pd.concat([data for _, data in test_subjects], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "#\n",
    "# # Group the data by subject and activity ID\n",
    "# grouped_data = sensor_data.groupby(['subjectID', 'activityID'], sort=False)\n",
    "#\n",
    "# # Split the data into train and test sets, stratifying across both subject and activity IDs\n",
    "# train_subjects, test_subjects = train_test_split(\n",
    "#     grouped_data, test_size=0.2, random_state=42, shuffle=True,\n",
    "#     stratify=grouped_data[['subjectID', 'activityID']]\n",
    "#         .apply(lambda x: '_'.join(x.astype(str)), axis=1))\n",
    "#\n",
    "# # Concatenate the train and test sets back into single dataframes\n",
    "# train_data = pd.concat([data for _, data in train_subjects], ignore_index=True, sort=False)\n",
    "# test_data = pd.concat([data for _, data in test_subjects], ignore_index=True, sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "#\n",
    "# # Group the data by subject and activity ID\n",
    "# grouped_data = sensor_data.groupby(['subjectID', 'activityID'], sort=False)\n",
    "#\n",
    "# # Concatenate the subjectID and activityID columns into a single column with '_' as separator\n",
    "# def concat_columns(x):\n",
    "#     return '_'.join(x.astype(str).values)\n",
    "#\n",
    "# grouped_data_str = grouped_data[['subjectID', 'activityID']].apply(concat_columns)\n",
    "#\n",
    "# # Split the data into train and test sets, stratifying across both subject and activity IDs\n",
    "# train_subjects, test_subjects = train_test_split(grouped_data, test_size=0.2, random_state=42, shuffle=True, stratify=grouped_data_str)\n",
    "#\n",
    "# # Concatenate the train and test sets back into single dataframes\n",
    "# train_data = pd.concat([data for _, data in train_subjects], ignore_index=True, sort=False)\n",
    "# test_data = pd.concat([data for _, data in test_subjects], ignore_index=True, sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Set the number of folds\n",
    "n_splits = 5\n",
    "\n",
    "# Create a time series split object\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Iterate over the folds\n",
    "for train_index, test_index in tscv.split(data):\n",
    "    # Split the data into train and test sets\n",
    "    train_data, test_data = data[train_index], data[test_index]\n",
    "\n",
    "    # Train and test the model\n",
    "    model.fit(train_data, train_labels, verbose=1)\n",
    "    test_preds = model.predict(test_data)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(test_labels, test_preds)\n",
    "    print(f\"Fold accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "def one_hot_encoding(sensor_data):\n",
    "    \"\"\"\n",
    "    Maps non-sequential activity IDs to sequential IDs and converts labels to one-hot encoding.\n",
    "\n",
    "    Args:\n",
    "    sensor_data: A Pandas dataframe containing sensor data.\n",
    "\n",
    "    Returns:\n",
    "    A modified Pandas dataframe with activity IDs mapped to sequential IDs and labels converted to one-hot encoding.\n",
    "    \"\"\"\n",
    "    label_map = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 12: 7, 13: 8, 16: 9, 17: 10, 24: 11}\n",
    "    sensor_data['activityID'] = sensor_data['activityID'].map(label_map)\n",
    "\n",
    "    num_classes = len(label_map)\n",
    "    labels = sensor_data['activityID'].values\n",
    "    sensor_data['activityID'] = to_categorical(labels, num_classes=num_classes)\n",
    "    print()\n",
    "    return sensor_data\n",
    "\n",
    "def extract_features(sensor_data):\n",
    "    \"\"\"\n",
    "    Extracts features from sensor data.\n",
    "\n",
    "    Args:\n",
    "    sensor_data: A Pandas dataframe containing sensor data.\n",
    "\n",
    "    Returns:\n",
    "    A tuple containing the feature matrix and labels array.\n",
    "    \"\"\"\n",
    "    # Define window size and step size\n",
    "    window_size = 100\n",
    "    step_size = 50\n",
    "\n",
    "    # Initialize an empty feature matrix and labels array\n",
    "    features = []\n",
    "    labels = []\n",
    "    print(sensor_data.shape)\n",
    "    # Slide the window over the data and extract features from each window\n",
    "    for i in range(0, len(sensor_data) - window_size, step_size):\n",
    "        # Extract the window of data\n",
    "        window = sensor_data.iloc[i:i+window_size]\n",
    "\n",
    "        # Extract the label of the window\n",
    "        label = window['activityID'].values[0]\n",
    "\n",
    "        # Check if the label is in the list of activities we want to classify\n",
    "        if label in protocol_acts:\n",
    "            # Check if the window contains only a single activity\n",
    "            if len(window['activityID'].unique()) == 1:\n",
    "                # Extract features from the window\n",
    "                window_features = []\n",
    "                for col in range(3, len(sensor_data.columns)): # exclude timestep, subjectID and activityID from feature extraction\n",
    "                    col_data = window.iloc[:, col].values\n",
    "                    window_features.append(np.mean(col_data))\n",
    "                    window_features.append(np.std(col_data))\n",
    "                    window_features.append(np.min(col_data))\n",
    "                    window_features.append(np.max(col_data))\n",
    "                    window_features.append(np.percentile(col_data, 25))\n",
    "                    window_features.append(np.percentile(col_data, 50))\n",
    "                    window_features.append(np.percentile(col_data, 75))\n",
    "\n",
    "                # Add the feature vector and label to the feature matrix and labels array\n",
    "                features.append(window_features)\n",
    "                labels.append(label)\n",
    "\n",
    "    # Convert the feature matrix and labels array to numpy arrays\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "sensor_data = one_hot_encoding(sensor_data)\n",
    "features, labels = extract_features(sensor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "windows without regard to true label\n",
    "classify by majority of activity performed\n",
    "cross validation\n",
    "apply rotation matrix to windows after classifying each\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print('Features\\n')\n",
    "print(features)\n",
    "print('\\nLabels\\n')\n",
    "print(labels)\n",
    "print(features.shape)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the input shape\n",
    "num_classes = len(set(labels))\n",
    "n_timesteps, n_features = X_train.shape[0], X_train.shape[1]\n",
    "\n",
    "# Reshape the input data\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], n_timesteps, n_features))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], n_timesteps, n_features))\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(n_timesteps, n_features)))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Test the model\n",
    "score, accuracy = model.evaluate(X_test, y_test, batch_size=64)\n",
    "print('Test loss:', score)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "    from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.utils import to_categorical\n",
    "#\n",
    "# # Convert the activity labels to one-hot vectors\n",
    "# labels_train_onehot = to_categorical(labels_train - 1, num_classes=num_activities)\n",
    "# labels_test_onehot = to_categorical(labels_test - 1, num_classes=num_activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Create the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(len(protocol_acts), activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "num_activities = 12\n",
    "window_size = 100\n",
    "num_features = 7\n",
    "# Define the input shape\n",
    "input_shape = (window_size, num_features)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=64, input_shape=input_shape))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=num_activities, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(features_train, labels_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(features_test, labels_test)\n",
    "print(\"Test loss:\", loss)\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Convert the activity labels to one-hot vectors\n",
    "labels_train_onehot = to_categorical(labels_train - 1, num_classes=num_activities)\n",
    "labels_test_onehot = to_categorical(labels_test - 1, num_classes=num_activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import LSTM, Dense\n",
    "#\n",
    "# # Define the LSTM model\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(64, input_shape=(window_size, num_features)))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "#\n",
    "# # Compile the model\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # Train the model\n",
    "# model.fit(train_features, train_labels, epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # Evaluate the model on the testing set\n",
    "# loss, accuracy = model.evaluate(test_features, test_labels)\n",
    "#\n",
    "# # Print the testing set accuracy\n",
    "# print('Testing set accuracy: {:.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# timestamp = df[0]\n",
    "# activity_id = df[1]\n",
    "# heart_rate = df[2]  # unlikely to be used\n",
    "#\n",
    "# imu_hand_temp = df.iloc[3] # unlikely to be used\n",
    "# imu_hand_acc_x = df.iloc[4]\n",
    "# imu_hand_acc_y = df.iloc[5]\n",
    "# imu_hand_acc_z = df.iloc[6]\n",
    "# imu_hand_gyro_x = df.iloc[10]\n",
    "# imu_hand_gyro_y = df.iloc[11]\n",
    "# imu_hand_gyro_z = df.iloc[12]\n",
    "#\n",
    "# imu_chest_temp = df.iloc[20] # unlikely to be used\n",
    "# imu_chest_acc_x = df.iloc[21]\n",
    "# imu_chest_acc_y = df.iloc[22]\n",
    "# imu_chest_acc_z = df.iloc[23]\n",
    "# imu_chest_gyro_x = df.iloc[27]\n",
    "# imu_chest_gyro_y = df.iloc[28]\n",
    "# imu_chest_gyro_z = df.iloc[29]\n",
    "#\n",
    "# imu_ankle_temp = df.iloc[37] # unlikely to be used\n",
    "# imu_ankle_acc_x = df.iloc[48]\n",
    "# imu_ankle_acc_y = df.iloc[39]\n",
    "# imu_ankle_acc_z = df.iloc[40]\n",
    "# imu_ankle_gyro_x = df.iloc[44]\n",
    "# imu_ankle_gyro_y = df.iloc[45]\n",
    "# imu_ankle_gyro_z = df.iloc[46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There are many higher-order features that can be computed. Some examples are:\n",
    "\n",
    "1. Time-domain features: These include mean, standard deviation, skewness, kurtosis, minimum, maximum, range, and interquartile range of the raw sensor data.\n",
    "2. Frequency-domain features: These include power spectral density, fast Fourier transform, and wavelet transform of the raw sensor data.\n",
    "Statistical features: These include mean crossing rate, autocorrelation, and cross-correlation of the raw sensor data.\n",
    "3. Signal quality features: These include signal-to-noise ratio, signal-to-artifact ratio, and signal-to-interference ratio of the raw sensor data.\n",
    "4. Heart rate variability: Such as time-domain, frequency-domain, and non-linear HRV features.\n",
    "5. Gait-related features: Such as step length, step width, step time, and stride time can be calculated for walking and running activity.\n",
    "6. Others: Some other examples include the calculation of entropy, fractal dimension, and information-theoretic features of the sensor data.\n",
    "\n",
    "It's important to note that feature extraction is a crucial step in the machine learning pipeline and the choice of features can greatly impact the model's performance. It's recommended to perform some feature selection or feature engineering to select the most relevant features for the task at hand."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Here's a sample code to perform windowing and feature extraction on a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataframe\n",
    "df = pd.DataFrame({\n",
    "    'subject_id': [1, 1, 1, 1, 1],\n",
    "    'activityID': [0, 0, 0, 0, 0],\n",
    "    'handAcc16_1': [2.37223, 2.18837, 2.37357, 2.07473, 2.22936],\n",
    "    'handAcc16_2': [8.60074, 8.5656, 8.60107, 8.52853, 8.83122],\n",
    "    'handAcc16_3': [3.51048, 3.66179, 3.54898, 3.66021, 3.7],\n",
    "    'handGyro1': [-0.0922174, -0.0244132, -0.0579761, -0.0023516, 0.0122691],\n",
    "    'handGyro2': [0.0568115, 0.0477585, 0.0325743, 0.0328098, 0.018305],\n",
    "    'handGyro3': [-0.0158445, 0.00647434, -0.00698815, -0.00374727, -0.0533248],\n",
    "    'chestAcc16_1': [0.23808, 0.31953, 0.235593, 0.388697, 0.3158],\n",
    "    'chestAcc16_2': [9.80003, 9.61282, 9.72421, 9.53572, 9.49908],\n",
    "    'chestAcc16_3': [-1.68896, -1.49328, -1.76621, -1.7241, -1.60914],\n",
    "    'chestGyro1': [-0.00506495, 0.013685, -0.0399232, 0.00751315, -0.00382244],\n",
    "    'chestGyro2': [-0.00678097, 0.00148646, 0.0340559, -0.0104983, -0.0112166],\n",
    "    'chestGyro3': [-0.00566295, -0.0415218, -0.002113, -0.020684, -0.0259745],\n",
    "    'ankleAcc16_1': [9.65918, 9.6937, 9.58944, 9.58814, 9.69771],\n",
    "    'ankleAcc16_2': [-1.65569, -1.57902, -1.73276, -1.7704, -1.65625],\n",
    "    'ankleAcc16_3': [-0.0997967, -0.215687, 0.0929141, 0.0545449, -0.0608086],\n",
    "    'ankleGyro1': [0.00830026, -0.00657665, 0.00301426, 0.00317498, 0.0126977],\n",
    "    'ankleGyro2': [0.00925038, -0.00463778, 0.000148236, -0.0203009, -0.0143027],\n",
    "    'ankleGyro3': [0.00925038, -0.00463778, 0.000148236, -0.0203009, -0.0143027]})\n",
    "\n",
    "def windowing_and_feature_extraction(df, window_size, overlap_ratio):\n",
    "    features = []\n",
    "    windows = []\n",
    "    n = df.shape[0]\n",
    "    shift = int(window_size * (1 - overlap_ratio))\n",
    "    for i in range(0, n - window_size + 1, shift):\n",
    "        window = df.iloc[i:i+window_size]\n",
    "        print(window)\n",
    "        windows.append(window)\n",
    "        mean = np.mean(window, axis=0)\n",
    "        std = np.std(window, axis=0)\n",
    "        min = np.min(window, axis=0)\n",
    "        max = np.max(window, axis=0)\n",
    "        features.append(np.concatenate([mean, std, min, max]))\n",
    "    return windows, np.array(features)\n",
    "\n",
    "# example usage\n",
    "windows, features = windowing_and_feature_extraction(df, window_size=100, overlap_ratio=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Note: The windowing_and_feature_extraction function takes three inputs:\n",
    "\n",
    "df: the original dataframe\n",
    "window_size: the size of the window to extract from the data\n",
    "overlap_ratio: the ratio of overlap between consecutive windows, expressed as a fraction between 0 and 1\n",
    "The function returns two outputs:\n",
    "\n",
    "windows: a list of sub-dataframes, each representing a window of the original data\n",
    "features: a 2D numpy array of shape (number of windows, number of features), where each row represents the extracted features from a window of the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(windows)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
